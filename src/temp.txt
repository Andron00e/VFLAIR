================= iter seed  60  =================
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: MID_Active
Defense_Config: {'party': [1], 'lr': 0.001, 'lambda': 0.01}
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/MID_Active_0.01,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
mid defense parties: [1]
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
mid defense parties: [1]
begin to load mid model for party 1
load global mid model for party 1,std_shift_hyperparameter=5
small MID model for nuswide
mid_lr = 0.001
validate and test
Epoch 0% 	 train_loss:1.12 train_acc:0.61 test_acc:0.67 test_auc:0.89
validate and test
Epoch 1% 	 train_loss:1.00 train_acc:0.67 test_acc:0.72 test_auc:0.92
validate and test
Epoch 2% 	 train_loss:0.96 train_acc:0.72 test_acc:0.75 test_auc:0.94
validate and test
Epoch 3% 	 train_loss:0.89 train_acc:0.76 test_acc:0.77 test_auc:0.94
validate and test
Epoch 4% 	 train_loss:0.79 train_acc:0.82 test_acc:0.80 test_auc:0.95
validate and test
Epoch 5% 	 train_loss:0.71 train_acc:0.86 test_acc:0.81 test_auc:0.96
validate and test
Epoch 6% 	 train_loss:0.63 train_acc:0.89 test_acc:0.82 test_auc:0.96
validate and test
Epoch 7% 	 train_loss:0.60 train_acc:0.90 test_acc:0.83 test_auc:0.96
validate and test
Epoch 8% 	 train_loss:0.57 train_acc:0.91 test_acc:0.84 test_auc:0.97
validate and test
Epoch 9% 	 train_loss:0.56 train_acc:0.90 test_acc:0.84 test_auc:0.97
validate and test
Epoch 10% 	 train_loss:0.55 train_acc:0.91 test_acc:0.84 test_auc:0.97
validate and test
Epoch 11% 	 train_loss:0.53 train_acc:0.92 test_acc:0.84 test_auc:0.97
validate and test
Epoch 12% 	 train_loss:0.52 train_acc:0.92 test_acc:0.84 test_auc:0.97
validate and test
Epoch 13% 	 train_loss:0.52 train_acc:0.92 test_acc:0.85 test_auc:0.97
validate and test
Epoch 14% 	 train_loss:0.51 train_acc:0.92 test_acc:0.85 test_auc:0.97
validate and test
Epoch 15% 	 train_loss:0.50 train_acc:0.93 test_acc:0.85 test_auc:0.97
validate and test
Epoch 16% 	 train_loss:0.49 train_acc:0.93 test_acc:0.85 test_auc:0.97
validate and test
Epoch 17% 	 train_loss:0.49 train_acc:0.93 test_acc:0.85 test_auc:0.97
validate and test
Epoch 18% 	 train_loss:0.48 train_acc:0.93 test_acc:0.85 test_auc:0.97
validate and test
Epoch 19% 	 train_loss:0.48 train_acc:0.93 test_acc:0.85 test_auc:0.97
validate and test
Epoch 20% 	 train_loss:0.47 train_acc:0.93 test_acc:0.85 test_auc:0.97
validate and test
Epoch 21% 	 train_loss:0.47 train_acc:0.93 test_acc:0.85 test_auc:0.97
validate and test
Epoch 22% 	 train_loss:0.46 train_acc:0.94 test_acc:0.85 test_auc:0.97
validate and test
Epoch 23% 	 train_loss:0.46 train_acc:0.93 test_acc:0.85 test_auc:0.97
validate and test
Epoch 24% 	 train_loss:0.46 train_acc:0.94 test_acc:0.85 test_auc:0.97
validate and test
Epoch 25% 	 train_loss:0.45 train_acc:0.94 test_acc:0.86 test_auc:0.97
validate and test
Epoch 26% 	 train_loss:0.45 train_acc:0.94 test_acc:0.85 test_auc:0.97
validate and test
Epoch 27% 	 train_loss:0.45 train_acc:0.95 test_acc:0.85 test_auc:0.97
validate and test
Epoch 28% 	 train_loss:0.44 train_acc:0.95 test_acc:0.86 test_auc:0.97
validate and test
Epoch 29% 	 train_loss:0.44 train_acc:0.95 test_acc:0.85 test_auc:0.97
validate and test
Epoch 30% 	 train_loss:0.44 train_acc:0.95 test_acc:0.85 test_auc:0.97
validate and test
Epoch 31% 	 train_loss:0.43 train_acc:0.95 test_acc:0.85 test_auc:0.97
validate and test
Epoch 32% 	 train_loss:0.43 train_acc:0.95 test_acc:0.85 test_auc:0.97
validate and test
Epoch 33% 	 train_loss:0.43 train_acc:0.95 test_acc:0.85 test_auc:0.97
validate and test
Epoch 34% 	 train_loss:0.42 train_acc:0.95 test_acc:0.85 test_auc:0.97
validate and test
Epoch 35% 	 train_loss:0.42 train_acc:0.95 test_acc:0.85 test_auc:0.97
validate and test
Epoch 36% 	 train_loss:0.42 train_acc:0.95 test_acc:0.85 test_auc:0.97
validate and test
Epoch 37% 	 train_loss:0.42 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 38% 	 train_loss:0.42 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 39% 	 train_loss:0.41 train_acc:0.95 test_acc:0.85 test_auc:0.97
validate and test
Epoch 40% 	 train_loss:0.41 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 41% 	 train_loss:0.41 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 42% 	 train_loss:0.41 train_acc:0.95 test_acc:0.85 test_auc:0.97
validate and test
Epoch 43% 	 train_loss:0.41 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 44% 	 train_loss:0.40 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 45% 	 train_loss:0.40 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 46% 	 train_loss:0.40 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 47% 	 train_loss:0.40 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 48% 	 train_loss:0.40 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 49% 	 train_loss:0.40 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 50% 	 train_loss:0.39 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 51% 	 train_loss:0.39 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 52% 	 train_loss:0.39 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 53% 	 train_loss:0.39 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 54% 	 train_loss:0.39 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 55% 	 train_loss:0.39 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 56% 	 train_loss:0.39 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 57% 	 train_loss:0.38 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 58% 	 train_loss:0.38 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 59% 	 train_loss:0.38 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 60% 	 train_loss:0.38 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 61% 	 train_loss:0.38 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 62% 	 train_loss:0.38 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 63% 	 train_loss:0.37 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 64% 	 train_loss:0.37 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 65% 	 train_loss:0.37 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 66% 	 train_loss:0.37 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 67% 	 train_loss:0.37 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 68% 	 train_loss:0.37 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 69% 	 train_loss:0.37 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 70% 	 train_loss:0.36 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 71% 	 train_loss:0.36 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 72% 	 train_loss:0.36 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 73% 	 train_loss:0.36 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 74% 	 train_loss:0.36 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 75% 	 train_loss:0.36 train_acc:0.96 test_acc:0.85 test_auc:0.97
validate and test
Epoch 76% 	 train_loss:0.36 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 77% 	 train_loss:0.36 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 78% 	 train_loss:0.36 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 79% 	 train_loss:0.35 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 80% 	 train_loss:0.35 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 81% 	 train_loss:0.35 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 82% 	 train_loss:0.35 train_acc:0.97 test_acc:0.85 test_auc:0.97
validate and test
Epoch 83% 	 train_loss:0.35 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 84% 	 train_loss:0.35 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 85% 	 train_loss:0.35 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 86% 	 train_loss:0.35 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 87% 	 train_loss:0.35 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 88% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 89% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 90% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 91% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 92% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 93% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 94% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 95% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 96% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 97% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 98% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
validate and test
Epoch 99% 	 train_loss:0.34 train_acc:0.97 test_acc:0.84 test_auc:0.97
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.84115|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
mid defense parties: [1]
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
mid defense parties: [1]
begin to load mid model for party 1
load global mid model for party 1,std_shift_hyperparameter=5
small MID model for nuswide
mid_lr = 0.001
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4470701502881862
batch_idx: 500  loss: 1.3536651725784252
batch_idx: 750  loss: 1.2988680854574552
batch_idx: 1000  loss: 1.2643958813370988
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6994750118255615, top 2 accuracy:0.8192500116825103

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9898146390914917
batch_idx: 250  loss: 1.157614602856871
batch_idx: 500  loss: 1.162119092600958
batch_idx: 750  loss: 1.151001675576844
batch_idx: 1000  loss: 1.151569339247367
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6926500132083893, top 2 accuracy:0.8295250129699707

Epoch: [3 | 20]
batch_idx: 0  loss: 1.2973493337631226
batch_idx: 250  loss: 1.1370885725620252
batch_idx: 500  loss: 1.154947975700932
batch_idx: 750  loss: 1.1586071800652686
batch_idx: 1000  loss: 1.1637065843604624
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6512500121593475, top 2 accuracy:0.8160250103473663

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4035617113113403
batch_idx: 250  loss: 1.166187569522706
batch_idx: 500  loss: 1.1714378945184856
batch_idx: 750  loss: 1.169532112234003
batch_idx: 1000  loss: 1.167364060331267
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6391750113964081, top 2 accuracy:0.8069000124931336

Epoch: [5 | 20]
batch_idx: 0  loss: 1.0819919109344482
batch_idx: 250  loss: 1.1929756785803645
batch_idx: 500  loss: 1.1982265918068529
batch_idx: 750  loss: 1.1869315829538423
batch_idx: 1000  loss: 1.1960379337302793
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6291500132083893, top 2 accuracy:0.8005500121116638

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2557649612426758
batch_idx: 250  loss: 1.1991271470423155
batch_idx: 500  loss: 1.2093924491219163
batch_idx: 750  loss: 1.2114717806191315
batch_idx: 1000  loss: 1.226144693720455
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6270000119209289, top 2 accuracy:0.8047000107765198

Epoch: [7 | 20]
batch_idx: 0  loss: 0.8996003866195679
batch_idx: 250  loss: 1.2627900364099511
batch_idx: 500  loss: 1.250126651457052
batch_idx: 750  loss: 1.2482761697543308
batch_idx: 1000  loss: 1.254378601575431
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6278750120401383, top 2 accuracy:0.809050012588501

Epoch: [8 | 20]
batch_idx: 0  loss: 0.8596842288970947
batch_idx: 250  loss: 1.2589561382992658
batch_idx: 500  loss: 1.2623751037048572
batch_idx: 750  loss: 1.2645219082588715
batch_idx: 1000  loss: 1.2693041022688436
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6214000115394592, top 2 accuracy:0.8080500106811523

Epoch: [9 | 20]
batch_idx: 0  loss: 1.405245065689087
batch_idx: 250  loss: 1.2723440819779717
batch_idx: 500  loss: 1.29947190773354
batch_idx: 750  loss: 1.3002625690171412
batch_idx: 1000  loss: 1.3123066336059341
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6142000135183334, top 2 accuracy:0.8091500110626221

Epoch: [10 | 20]
batch_idx: 0  loss: 1.2788763046264648
batch_idx: 250  loss: 1.2883224322420614
batch_idx: 500  loss: 1.313694309294699
batch_idx: 750  loss: 1.3155277445952014
batch_idx: 1000  loss: 1.3304215861490358
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.619350011587143, top 2 accuracy:0.8120750119686126

Epoch: [11 | 20]
batch_idx: 0  loss: 1.4880927801132202
batch_idx: 250  loss: 1.3444654951982544
batch_idx: 500  loss: 1.3381011094868278
batch_idx: 750  loss: 1.3380336047741503
batch_idx: 1000  loss: 1.3427979544328805
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6196250133514404, top 2 accuracy:0.8131750106811524

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1135951280593872
batch_idx: 250  loss: 1.3322023083181942
batch_idx: 500  loss: 1.3462135707172291
batch_idx: 750  loss: 1.3609215038897446
batch_idx: 1000  loss: 1.359619394087563
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6125500106811523, top 2 accuracy:0.8122250142097474

Epoch: [13 | 20]
batch_idx: 0  loss: 1.3845579624176025
batch_idx: 250  loss: 1.381976732388968
batch_idx: 500  loss: 1.378960019093381
batch_idx: 750  loss: 1.3845015286193876
batch_idx: 1000  loss: 1.3822864344754158
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6019250106811523, top 2 accuracy:0.8126750128269196

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0779993534088135
batch_idx: 250  loss: 1.40255916431333
batch_idx: 500  loss: 1.4079544980560192
batch_idx: 750  loss: 1.414640042457002
batch_idx: 1000  loss: 1.4166677360193798
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.596725011587143, top 2 accuracy:0.8117500112056732

Epoch: [15 | 20]
batch_idx: 0  loss: 1.330128788948059
batch_idx: 250  loss: 1.4396450347096819
batch_idx: 500  loss: 1.452455659992577
batch_idx: 750  loss: 1.4466519504230413
batch_idx: 1000  loss: 1.4455617456771315
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5823250105381012, top 2 accuracy:0.81135001206398

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3684941530227661
batch_idx: 250  loss: 1.4316517815870398
batch_idx: 500  loss: 1.4570311498033564
batch_idx: 750  loss: 1.4695649174314156
batch_idx: 1000  loss: 1.4651157954535163
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5558500126600265, top 2 accuracy:0.8078250105381012

Epoch: [17 | 20]
batch_idx: 0  loss: 1.2300916910171509
batch_idx: 250  loss: 1.4398543585464951
batch_idx: 500  loss: 1.4588420829799567
batch_idx: 750  loss: 1.4643573522123647
batch_idx: 1000  loss: 1.4703103417453294
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5457250115871429, top 2 accuracy:0.808125010728836

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8748772144317627
batch_idx: 250  loss: 1.5135471743127311
batch_idx: 500  loss: 1.491432046871246
batch_idx: 750  loss: 1.5074102037317134
batch_idx: 1000  loss: 1.506216675900042
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.4796750073432922, top 2 accuracy:0.8057750113010407

Epoch: [19 | 20]
batch_idx: 0  loss: 1.1896049976348877
batch_idx: 250  loss: 1.5653535317161693
batch_idx: 500  loss: 1.5449407428645632
batch_idx: 750  loss: 1.5558874696540732
batch_idx: 1000  loss: 1.5661551049246956
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.3940250052213669, top 2 accuracy:0.8018000128269196

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9200260639190674
batch_idx: 250  loss: 1.5777736934455666
batch_idx: 500  loss: 1.594270217932012
batch_idx: 750  loss: 1.6089615754271644
batch_idx: 1000  loss: 1.6162985403078813
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.28895000559091566, top 2 accuracy:0.794650012254715
MC, if self.args.apply_defense=True
MC Best top 1 accuracy: 0.6278750120401383
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.84115|0.6278750120401383
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
mid defense parties: [1]
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
mid defense parties: [1]
begin to load mid model for party 1
load global mid model for party 1,std_shift_hyperparameter=5
small MID model for nuswide
mid_lr = 0.001
validate and test
Epoch 0% 	 train_loss:1.07 train_acc:0.61 test_acc:0.67 test_auc:0.89
validate and test
Epoch 1% 	 train_loss:0.84 train_acc:0.79 test_acc:0.77 test_auc:0.94
validate and test
Epoch 2% 	 train_loss:0.70 train_acc:0.85 test_acc:0.80 test_auc:0.95
validate and test
Epoch 3% 	 train_loss:0.59 train_acc:0.89 test_acc:0.83 test_auc:0.96
validate and test
Epoch 4% 	 train_loss:0.55 train_acc:0.90 test_acc:0.85 test_auc:0.97
validate and test
Epoch 5% 	 train_loss:0.54 train_acc:0.91 test_acc:0.86 test_auc:0.97
validate and test
Epoch 6% 	 train_loss:0.53 train_acc:0.91 test_acc:0.87 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.52 train_acc:0.91 test_acc:0.87 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.52 train_acc:0.91 test_acc:0.87 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.52 train_acc:0.92 test_acc:0.87 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.51 train_acc:0.92 test_acc:0.87 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.51 train_acc:0.92 test_acc:0.87 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.50 train_acc:0.92 test_acc:0.87 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.50 train_acc:0.93 test_acc:0.87 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.50 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.50 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.49 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.49 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.48 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.48 train_acc:0.94 test_acc:0.88 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.47 train_acc:0.94 test_acc:0.88 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.47 train_acc:0.94 test_acc:0.88 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.46 train_acc:0.94 test_acc:0.88 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.46 train_acc:0.94 test_acc:0.88 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.45 train_acc:0.94 test_acc:0.88 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.45 train_acc:0.94 test_acc:0.88 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.45 train_acc:0.94 test_acc:0.88 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.44 train_acc:0.95 test_acc:0.88 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.44 train_acc:0.95 test_acc:0.88 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.43 train_acc:0.94 test_acc:0.87 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.43 train_acc:0.95 test_acc:0.87 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.43 train_acc:0.95 test_acc:0.87 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.42 train_acc:0.95 test_acc:0.87 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.42 train_acc:0.95 test_acc:0.87 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.42 train_acc:0.95 test_acc:0.87 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.41 train_acc:0.95 test_acc:0.87 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.41 train_acc:0.95 test_acc:0.87 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.40 train_acc:0.96 test_acc:0.87 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.40 train_acc:0.95 test_acc:0.87 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.40 train_acc:0.95 test_acc:0.87 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.40 train_acc:0.96 test_acc:0.87 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.39 train_acc:0.95 test_acc:0.87 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.39 train_acc:0.96 test_acc:0.87 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.39 train_acc:0.97 test_acc:0.87 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.39 train_acc:0.95 test_acc:0.87 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.39 train_acc:0.96 test_acc:0.87 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.38 train_acc:0.97 test_acc:0.87 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.38 train_acc:0.96 test_acc:0.87 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.38 train_acc:0.97 test_acc:0.87 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.38 train_acc:0.97 test_acc:0.87 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.38 train_acc:0.96 test_acc:0.87 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.38 train_acc:0.97 test_acc:0.87 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.37 train_acc:0.97 test_acc:0.87 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.37 train_acc:0.96 test_acc:0.87 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.37 train_acc:0.97 test_acc:0.87 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.37 train_acc:0.97 test_acc:0.87 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.37 train_acc:0.96 test_acc:0.87 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.36 train_acc:0.97 test_acc:0.87 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.36 train_acc:0.97 test_acc:0.87 test_auc:0.97
validate and test
Epoch 59% 	 train_loss:0.36 train_acc:0.97 test_acc:0.87 test_auc:0.97
validate and test
Epoch 60% 	 train_loss:0.36 train_acc:0.97 test_acc:0.87 test_auc:0.97
validate and test
Epoch 61% 	 train_loss:0.36 train_acc:0.97 test_acc:0.87 test_auc:0.97
validate and test
Epoch 62% 	 train_loss:0.36 train_acc:0.97 test_acc:0.87 test_auc:0.97
validate and test
Epoch 63% 	 train_loss:0.36 train_acc:0.97 test_acc:0.87 test_auc:0.97
validate and test
Epoch 64% 	 train_loss:0.36 train_acc:0.97 test_acc:0.87 test_auc:0.97
validate and test
Epoch 65% 	 train_loss:0.36 train_acc:0.97 test_acc:0.86 test_auc:0.97
validate and test
Epoch 66% 	 train_loss:0.35 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 67% 	 train_loss:0.35 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 68% 	 train_loss:0.35 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 69% 	 train_loss:0.35 train_acc:0.97 test_acc:0.86 test_auc:0.97
validate and test
Epoch 70% 	 train_loss:0.35 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 71% 	 train_loss:0.35 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 72% 	 train_loss:0.35 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 73% 	 train_loss:0.34 train_acc:0.97 test_acc:0.86 test_auc:0.97
validate and test
Epoch 74% 	 train_loss:0.34 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 75% 	 train_loss:0.34 train_acc:0.97 test_acc:0.86 test_auc:0.97
validate and test
Epoch 76% 	 train_loss:0.34 train_acc:0.97 test_acc:0.86 test_auc:0.97
validate and test
Epoch 77% 	 train_loss:0.34 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 78% 	 train_loss:0.34 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 79% 	 train_loss:0.34 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 80% 	 train_loss:0.34 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 81% 	 train_loss:0.34 train_acc:0.97 test_acc:0.86 test_auc:0.97
validate and test
Epoch 82% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 83% 	 train_loss:0.34 train_acc:0.97 test_acc:0.86 test_auc:0.97
validate and test
Epoch 84% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 85% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 86% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 87% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 88% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 89% 	 train_loss:0.33 train_acc:0.97 test_acc:0.86 test_auc:0.97
validate and test
Epoch 90% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 91% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 92% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 93% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 94% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 95% 	 train_loss:0.33 train_acc:0.97 test_acc:0.86 test_auc:0.97
validate and test
Epoch 96% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 97% 	 train_loss:0.33 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 98% 	 train_loss:0.32 train_acc:0.98 test_acc:0.86 test_auc:0.97
validate and test
Epoch 99% 	 train_loss:0.32 train_acc:0.98 test_acc:0.86 test_auc:0.97
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4919646487516516
batch_idx: 500  loss: 1.423784727067278
batch_idx: 750  loss: 1.38263414237904
batch_idx: 1000  loss: 1.3535590310827992
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5339500107765198, top 2 accuracy:0.7517750105857849

Epoch: [2 | 20]
batch_idx: 0  loss: 1.156557559967041
batch_idx: 250  loss: 1.2483783320705917
batch_idx: 500  loss: 1.2527501953370643
batch_idx: 750  loss: 1.2424297834280136
batch_idx: 1000  loss: 1.2441369953532568
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5869000135660172, top 2 accuracy:0.7369000120162964

Epoch: [3 | 20]
batch_idx: 0  loss: 1.5268617868423462
batch_idx: 250  loss: 1.2249105162385536
batch_idx: 500  loss: 1.23497132099036
batch_idx: 750  loss: 1.2400855918143772
batch_idx: 1000  loss: 1.246429966447262
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5948000127077102, top 2 accuracy:0.7470000116825104

Epoch: [4 | 20]
batch_idx: 0  loss: 1.5025256872177124
batch_idx: 250  loss: 1.2621384704624716
batch_idx: 500  loss: 1.258081793072121
batch_idx: 750  loss: 1.2561825746675321
batch_idx: 1000  loss: 1.2537192619932345
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5962500112056732, top 2 accuracy:0.746300012588501

Epoch: [5 | 20]
batch_idx: 0  loss: 1.166979193687439
batch_idx: 250  loss: 1.2639641273760833
batch_idx: 500  loss: 1.2694799439948903
batch_idx: 750  loss: 1.2632876311990413
batch_idx: 1000  loss: 1.269906495587704
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5965000120401382, top 2 accuracy:0.7440500106811524

Epoch: [6 | 20]
batch_idx: 0  loss: 1.3457576036453247
batch_idx: 250  loss: 1.2803354706786965
batch_idx: 500  loss: 1.2916069036465512
batch_idx: 750  loss: 1.289454111239579
batch_idx: 1000  loss: 1.2984371823719896
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5982250120639802, top 2 accuracy:0.7470000116825104

Epoch: [7 | 20]
batch_idx: 0  loss: 1.0442198514938354
batch_idx: 250  loss: 1.321434638079475
batch_idx: 500  loss: 1.31640383905391
batch_idx: 750  loss: 1.3114148872275502
batch_idx: 1000  loss: 1.3177039220024602
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5883750120401382, top 2 accuracy:0.743375013589859

Epoch: [8 | 20]
batch_idx: 0  loss: 1.037010669708252
batch_idx: 250  loss: 1.3300604114282302
batch_idx: 500  loss: 1.3261721822918888
batch_idx: 750  loss: 1.3269064217377622
batch_idx: 1000  loss: 1.3313826962638968
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5844750125408172, top 2 accuracy:0.7405750117301941

Epoch: [9 | 20]
batch_idx: 0  loss: 1.5683262348175049
batch_idx: 250  loss: 1.3378577398192522
batch_idx: 500  loss: 1.3609584353186868
batch_idx: 750  loss: 1.36246182478889
batch_idx: 1000  loss: 1.371939899370122
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.582825012087822, top 2 accuracy:0.7362750122547149

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3731896877288818
batch_idx: 250  loss: 1.3504368995445142
batch_idx: 500  loss: 1.3719972515125214
batch_idx: 750  loss: 1.3744490491132142
batch_idx: 1000  loss: 1.386540994928858
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5816250122785568, top 2 accuracy:0.7366500120162964

Epoch: [11 | 20]
batch_idx: 0  loss: 1.4633681774139404
batch_idx: 250  loss: 1.3986736312390131
batch_idx: 500  loss: 1.3901057920672677
batch_idx: 750  loss: 1.391177699723226
batch_idx: 1000  loss: 1.3953067826720091
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5696500108242035, top 2 accuracy:0.7337750110626221

Epoch: [12 | 20]
batch_idx: 0  loss: 1.244715690612793
batch_idx: 250  loss: 1.385281749673792
batch_idx: 500  loss: 1.3973410427095028
batch_idx: 750  loss: 1.41342929483031
batch_idx: 1000  loss: 1.4087467799647548
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5625750126838684, top 2 accuracy:0.7263500130176545

Epoch: [13 | 20]
batch_idx: 0  loss: 1.346677303314209
batch_idx: 250  loss: 1.418694752767469
batch_idx: 500  loss: 1.4194874849045676
batch_idx: 750  loss: 1.4270734780325796
batch_idx: 1000  loss: 1.424562478289246
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5668750107288361, top 2 accuracy:0.7298750112056732

Epoch: [14 | 20]
batch_idx: 0  loss: 1.1453620195388794
batch_idx: 250  loss: 1.4415595446178757
batch_idx: 500  loss: 1.4478304037256864
batch_idx: 750  loss: 1.4579833665383883
batch_idx: 1000  loss: 1.4628275220338909
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.552975010752678, top 2 accuracy:0.7244000127315521

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3079216480255127
batch_idx: 250  loss: 1.5011145026202422
batch_idx: 500  loss: 1.4983819906601306
batch_idx: 750  loss: 1.4908865309890882
batch_idx: 1000  loss: 1.488800665512443
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5331000118255615, top 2 accuracy:0.7213500125408172

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3916298151016235
batch_idx: 250  loss: 1.4753486358683514
batch_idx: 500  loss: 1.5041834655465882
batch_idx: 750  loss: 1.5231618281689776
batch_idx: 1000  loss: 1.5214963278022056
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.4519500073194504, top 2 accuracy:0.7258000111579895

Epoch: [17 | 20]
batch_idx: 0  loss: 1.2090591192245483
batch_idx: 250  loss: 1.4993175275754094
batch_idx: 500  loss: 1.5026022684441038
batch_idx: 750  loss: 1.5133169261682153
batch_idx: 1000  loss: 1.5264822278921597
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.4126250057220459, top 2 accuracy:0.7288500094413757

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8506603240966797
batch_idx: 250  loss: 1.6306135499041485
batch_idx: 500  loss: 1.6285764386304067
batch_idx: 750  loss: 1.6567864185828614
batch_idx: 1000  loss: 1.6585874696509144
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.261775004029274, top 2 accuracy:0.7274500110149383

Epoch: [19 | 20]
batch_idx: 0  loss: 1.2766151428222656
batch_idx: 250  loss: 1.7007835821051667
batch_idx: 500  loss: 1.6979496609271048
batch_idx: 750  loss: 1.7110565569630451
batch_idx: 1000  loss: 1.7194905650501433
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.20950000309944153, top 2 accuracy:0.7331000106334686

Epoch: [20 | 20]
batch_idx: 0  loss: 2.064805746078491
batch_idx: 250  loss: 1.7229059349940956
batch_idx: 500  loss: 1.7401453714336506
batch_idx: 750  loss: 1.7538886079133478
batch_idx: 1000  loss: 1.758307064826877
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.16912500265240668, top 2 accuracy:0.7304750115871429
MC, if self.args.apply_defense=True
MC Best top 1 accuracy: 0.5883750120401382
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.85555|0.5883750120401382
