================= iter seed  97  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.66 train_acc:0.76 test_acc:0.81 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.41 train_acc:0.88 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.34 train_acc:0.89 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.30 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.27 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.25 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.24 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.20 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.19 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.18 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.18 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.18 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.18 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.18 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.15 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.15 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.15 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.15 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.12 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.11 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.11 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.11 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.11 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.11 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.883925|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4142389231152679
batch_idx: 500  loss: 1.2998327193172734
batch_idx: 750  loss: 1.2477446137558215
batch_idx: 1000  loss: 1.215129731455074
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7544000113010406, top 2 accuracy:0.8748250133991241

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9260318875312805
batch_idx: 250  loss: 1.1250860049538847
batch_idx: 500  loss: 1.1380059200515777
batch_idx: 750  loss: 1.1320379763413388
batch_idx: 1000  loss: 1.1371301312368518
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7120000128746032, top 2 accuracy:0.8406750111579895

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4393742084503174
batch_idx: 250  loss: 1.1270260192062973
batch_idx: 500  loss: 1.1456057866604514
batch_idx: 750  loss: 1.1550276202049072
batch_idx: 1000  loss: 1.1602107403329767
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6918250117301941, top 2 accuracy:0.8286500120162964

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4040451049804688
batch_idx: 250  loss: 1.1750203199530633
batch_idx: 500  loss: 1.1757578085959433
batch_idx: 750  loss: 1.1736267235699076
batch_idx: 1000  loss: 1.170750819170437
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6812250111103058, top 2 accuracy:0.8211250112056733

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1592117547988892
batch_idx: 250  loss: 1.194214381455238
batch_idx: 500  loss: 1.1971067279529723
batch_idx: 750  loss: 1.1869215230284504
batch_idx: 1000  loss: 1.1924513617929178
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6740000104904175, top 2 accuracy:0.8191500124931336

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2630510330200195
batch_idx: 250  loss: 1.197822433965953
batch_idx: 500  loss: 1.205932996489785
batch_idx: 750  loss: 1.2032793320322874
batch_idx: 1000  loss: 1.216040634094907
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6773250110149384, top 2 accuracy:0.8216500127315521

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9379749298095703
batch_idx: 250  loss: 1.2512846495275087
batch_idx: 500  loss: 1.242893781673395
batch_idx: 750  loss: 1.2404031703420115
batch_idx: 1000  loss: 1.2433692739365962
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6696750133037567, top 2 accuracy:0.8195750114917755

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9090874195098877
batch_idx: 250  loss: 1.249587425935439
batch_idx: 500  loss: 1.2494434705286315
batch_idx: 750  loss: 1.2499840846399224
batch_idx: 1000  loss: 1.2555469414487053
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6681000115871429, top 2 accuracy:0.8199500114917755

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3795175552368164
batch_idx: 250  loss: 1.2639740082910595
batch_idx: 500  loss: 1.2836858857856792
batch_idx: 750  loss: 1.2849286374670195
batch_idx: 1000  loss: 1.2969950597031048
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6600750114917755, top 2 accuracy:0.8187250101566315

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3161370754241943
batch_idx: 250  loss: 1.2841834707366264
batch_idx: 500  loss: 1.2992634379692625
batch_idx: 750  loss: 1.299616096944743
batch_idx: 1000  loss: 1.3115611429840992
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6528250110149384, top 2 accuracy:0.813775013923645

Epoch: [11 | 20]
batch_idx: 0  loss: 1.300032377243042
batch_idx: 250  loss: 1.3245503569824328
batch_idx: 500  loss: 1.3238915002041836
batch_idx: 750  loss: 1.3241927236429614
batch_idx: 1000  loss: 1.3253292662029068
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6482750136852264, top 2 accuracy:0.8064750118255615

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1994600296020508
batch_idx: 250  loss: 1.3105021862991284
batch_idx: 500  loss: 1.3245484799004057
batch_idx: 750  loss: 1.3401207008407496
batch_idx: 1000  loss: 1.3372511259568767
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6408250114917755, top 2 accuracy:0.8032750117778779

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2952704429626465
batch_idx: 250  loss: 1.3499933568396516
batch_idx: 500  loss: 1.3513488109792438
batch_idx: 750  loss: 1.3598605347098411
batch_idx: 1000  loss: 1.3549131305929951
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6400500118732453, top 2 accuracy:0.8034750142097473

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0260560512542725
batch_idx: 250  loss: 1.375993977859024
batch_idx: 500  loss: 1.379289927712658
batch_idx: 750  loss: 1.3886292712557753
batch_idx: 1000  loss: 1.391816111918265
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.647250010728836, top 2 accuracy:0.8053500118255615

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3706854581832886
batch_idx: 250  loss: 1.4197179647621556
batch_idx: 500  loss: 1.4179479512111233
batch_idx: 750  loss: 1.4121316906110348
batch_idx: 1000  loss: 1.4120494904942786
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.634650012254715, top 2 accuracy:0.7989750108718872

Epoch: [16 | 20]
batch_idx: 0  loss: 1.2918578386306763
batch_idx: 250  loss: 1.3955730026401283
batch_idx: 500  loss: 1.4175351335766593
batch_idx: 750  loss: 1.431424591184741
batch_idx: 1000  loss: 1.4322422333418752
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5970250122547149, top 2 accuracy:0.7829250121116638

Epoch: [17 | 20]
batch_idx: 0  loss: 1.146700382232666
batch_idx: 250  loss: 1.4220423677577883
batch_idx: 500  loss: 1.4326234405690974
batch_idx: 750  loss: 1.4337999525065115
batch_idx: 1000  loss: 1.4411835847333216
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6140000104904175, top 2 accuracy:0.7852250130176545

Epoch: [18 | 20]
batch_idx: 0  loss: 1.7951295375823975
batch_idx: 250  loss: 1.4813230348505162
batch_idx: 500  loss: 1.4542551061563325
batch_idx: 750  loss: 1.4726976616196077
batch_idx: 1000  loss: 1.481101400364702
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5174000092744827, top 2 accuracy:0.7692750120162963

Epoch: [19 | 20]
batch_idx: 0  loss: 1.0828282833099365
batch_idx: 250  loss: 1.533475067259208
batch_idx: 500  loss: 1.5286318737258942
batch_idx: 750  loss: 1.541014269636691
batch_idx: 1000  loss: 1.550473873583844
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.41250000619888305, top 2 accuracy:0.7692250120639801

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9357411861419678
batch_idx: 250  loss: 1.571604488763976
batch_idx: 500  loss: 1.5730394560374332
batch_idx: 750  loss: 1.58504556486618
batch_idx: 1000  loss: 1.5884007635874489
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.34757500517368317, top 2 accuracy:0.7723500127792359
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6696750133037567
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.883925|0.6696750133037567
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.49 train_acc:0.83 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.31 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.26 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.24 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.22 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.21 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4284358218857138
batch_idx: 500  loss: 1.3183700265971858
batch_idx: 750  loss: 1.269026477898988
batch_idx: 1000  loss: 1.237726137208672
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7295500125885009, top 2 accuracy:0.8643250124454498

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9448686838150024
batch_idx: 250  loss: 1.1572901179187818
batch_idx: 500  loss: 1.1702458442113046
batch_idx: 750  loss: 1.163604777375709
batch_idx: 1000  loss: 1.1684397872977745
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.692375011920929, top 2 accuracy:0.8257750134468078

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4630886316299438
batch_idx: 250  loss: 1.1546785882622335
batch_idx: 500  loss: 1.1736457325530014
batch_idx: 750  loss: 1.1828240387740447
batch_idx: 1000  loss: 1.1877603193108266
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6723250095844269, top 2 accuracy:0.8126250109672546

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4373064041137695
batch_idx: 250  loss: 1.2014021293537036
batch_idx: 500  loss: 1.2026297553209218
batch_idx: 750  loss: 1.200517562617007
batch_idx: 1000  loss: 1.1972887570770403
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6638000102043152, top 2 accuracy:0.8110000109672546

Epoch: [5 | 20]
batch_idx: 0  loss: 1.196730613708496
batch_idx: 250  loss: 1.220395033613487
batch_idx: 500  loss: 1.2234550237750703
batch_idx: 750  loss: 1.2128512050446962
batch_idx: 1000  loss: 1.2186830923104057
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6552750113010406, top 2 accuracy:0.8085750119686127

Epoch: [6 | 20]
batch_idx: 0  loss: 1.288074016571045
batch_idx: 250  loss: 1.2227132638800884
batch_idx: 500  loss: 1.2305353916527932
batch_idx: 750  loss: 1.2282588340108098
batch_idx: 1000  loss: 1.2405472053363682
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6577750105857849, top 2 accuracy:0.8132250111103058

Epoch: [7 | 20]
batch_idx: 0  loss: 0.967265248298645
batch_idx: 250  loss: 1.277172147280082
batch_idx: 500  loss: 1.2664470320398158
batch_idx: 750  loss: 1.2641078970291437
batch_idx: 1000  loss: 1.267053474490635
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6511500136852264, top 2 accuracy:0.8119500124454498

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9595950245857239
batch_idx: 250  loss: 1.2743846308824935
batch_idx: 500  loss: 1.27319573529029
batch_idx: 750  loss: 1.273597422529244
batch_idx: 1000  loss: 1.2790753943994404
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6523500123023986, top 2 accuracy:0.8144500138759613

Epoch: [9 | 20]
batch_idx: 0  loss: 1.409326195716858
batch_idx: 250  loss: 1.288144711375426
batch_idx: 500  loss: 1.3070341389049944
batch_idx: 750  loss: 1.3105751745494012
batch_idx: 1000  loss: 1.322073065410978
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.648325012922287, top 2 accuracy:0.8140750126838684

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3942546844482422
batch_idx: 250  loss: 1.3075463660949744
batch_idx: 500  loss: 1.3221146607893315
batch_idx: 750  loss: 1.3230063303662716
batch_idx: 1000  loss: 1.3348648654291044
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6450750119686127, top 2 accuracy:0.810375013589859

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2992751598358154
batch_idx: 250  loss: 1.3461508091379235
batch_idx: 500  loss: 1.3463877600155758
batch_idx: 750  loss: 1.348013956093801
batch_idx: 1000  loss: 1.349004150746158
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.640600013256073, top 2 accuracy:0.8046250095367432

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1936314105987549
batch_idx: 250  loss: 1.333334586578636
batch_idx: 500  loss: 1.3476771718006955
batch_idx: 750  loss: 1.3628965299645912
batch_idx: 1000  loss: 1.3607672918052338
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6370500123500824, top 2 accuracy:0.8027750115394592

Epoch: [13 | 20]
batch_idx: 0  loss: 1.3124617338180542
batch_idx: 250  loss: 1.37598940147315
batch_idx: 500  loss: 1.3750932905377384
batch_idx: 750  loss: 1.3833594395798403
batch_idx: 1000  loss: 1.3789770068785252
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6409000122547149, top 2 accuracy:0.8049250106811523

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0687782764434814
batch_idx: 250  loss: 1.4012932730024303
batch_idx: 500  loss: 1.4051234259654841
batch_idx: 750  loss: 1.4145679108762816
batch_idx: 1000  loss: 1.4170682088207132
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.649700012922287, top 2 accuracy:0.8074000117778778

Epoch: [15 | 20]
batch_idx: 0  loss: 1.4007341861724854
batch_idx: 250  loss: 1.4411784230430857
batch_idx: 500  loss: 1.4403309043514672
batch_idx: 750  loss: 1.4360204140133779
batch_idx: 1000  loss: 1.4360675274754484
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6416500127315521, top 2 accuracy:0.8038750109672547

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3146350383758545
batch_idx: 250  loss: 1.4178358299364535
batch_idx: 500  loss: 1.4379617810439456
batch_idx: 750  loss: 1.4511666280783002
batch_idx: 1000  loss: 1.4531294475443446
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6203500134944916, top 2 accuracy:0.7878750140666961

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1870088577270508
batch_idx: 250  loss: 1.4475875530030655
batch_idx: 500  loss: 1.4589957479845013
batch_idx: 750  loss: 1.4600601558548265
batch_idx: 1000  loss: 1.4666535119993238
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6303500108718872, top 2 accuracy:0.7900750126838684

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8318946361541748
batch_idx: 250  loss: 1.5031105666547209
batch_idx: 500  loss: 1.475783136805469
batch_idx: 750  loss: 1.4846287527066586
batch_idx: 1000  loss: 1.4848247589632726
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5615750119686127, top 2 accuracy:0.7670500128269195

Epoch: [19 | 20]
batch_idx: 0  loss: 1.134276270866394
batch_idx: 250  loss: 1.511151951712531
batch_idx: 500  loss: 1.50362817864669
batch_idx: 750  loss: 1.5114108155927104
batch_idx: 1000  loss: 1.51315808696107
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5603750100135804, top 2 accuracy:0.7700250127315521

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9407083988189697
batch_idx: 250  loss: 1.5156262641112261
batch_idx: 500  loss: 1.5134164397225995
batch_idx: 750  loss: 1.5208894175503596
batch_idx: 1000  loss: 1.5196274745102507
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6030500127077103, top 2 accuracy:0.767400013923645
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6523500123023986
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.8813|0.6523500123023986
================= iter seed  98  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.67 train_acc:0.75 test_acc:0.80 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.41 train_acc:0.87 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.34 train_acc:0.89 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.30 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.27 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.25 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.24 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.20 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.88335|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.428227186203003
batch_idx: 500  loss: 1.3137093052814641
batch_idx: 750  loss: 1.260154479263056
batch_idx: 1000  loss: 1.225625792940775
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7258500115871429, top 2 accuracy:0.8714500122070312

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9649686217308044
batch_idx: 250  loss: 1.1389302272106772
batch_idx: 500  loss: 1.1495304324410178
batch_idx: 750  loss: 1.1433581091610279
batch_idx: 1000  loss: 1.1498785135321343
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6939750111103058, top 2 accuracy:0.8376750135421753

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4590080976486206
batch_idx: 250  loss: 1.1388607068736527
batch_idx: 500  loss: 1.1551097042062446
batch_idx: 750  loss: 1.1627744696380105
batch_idx: 1000  loss: 1.1679594257531074
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.677125011920929, top 2 accuracy:0.8283500113487243

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4149084091186523
batch_idx: 250  loss: 1.1851390776459856
batch_idx: 500  loss: 1.1860018840246793
batch_idx: 750  loss: 1.1840655041413461
batch_idx: 1000  loss: 1.1813997742705071
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6749750127792359, top 2 accuracy:0.8274750120639801

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1765774488449097
batch_idx: 250  loss: 1.2043580493411503
batch_idx: 500  loss: 1.2072306488784688
batch_idx: 750  loss: 1.1969104350850328
batch_idx: 1000  loss: 1.2023125744808596
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6734750139713287, top 2 accuracy:0.8266250109672546

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2680569887161255
batch_idx: 250  loss: 1.2110128471885462
batch_idx: 500  loss: 1.216634127702059
batch_idx: 750  loss: 1.2143393915469751
batch_idx: 1000  loss: 1.2266589863755453
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6790750131607056, top 2 accuracy:0.8309000105857849

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9604387283325195
batch_idx: 250  loss: 1.2638163662297928
batch_idx: 500  loss: 1.2552871138950665
batch_idx: 750  loss: 1.2527017338025945
batch_idx: 1000  loss: 1.2551910777251942
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6759000124931336, top 2 accuracy:0.8298250110149383

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9105570912361145
batch_idx: 250  loss: 1.2614052335484418
batch_idx: 500  loss: 1.2606218833672374
batch_idx: 750  loss: 1.2617276014324958
batch_idx: 1000  loss: 1.2674943122286766
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6794750125408172, top 2 accuracy:0.8320000109672546

Epoch: [9 | 20]
batch_idx: 0  loss: 1.337743878364563
batch_idx: 250  loss: 1.2771313511699864
batch_idx: 500  loss: 1.294641688489838
batch_idx: 750  loss: 1.2966331712992791
batch_idx: 1000  loss: 1.3087179848370842
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6793500106334687, top 2 accuracy:0.8331750106811523

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3658685684204102
batch_idx: 250  loss: 1.2981795499927478
batch_idx: 500  loss: 1.3151048133818157
batch_idx: 750  loss: 1.3150585181475827
batch_idx: 1000  loss: 1.3268756304924099
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6764500112533569, top 2 accuracy:0.8304750113487244

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2797942161560059
batch_idx: 250  loss: 1.3383287923893223
batch_idx: 500  loss: 1.337259686592093
batch_idx: 750  loss: 1.3379253404073224
batch_idx: 1000  loss: 1.3385615690400043
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6766750116348267, top 2 accuracy:0.8264500102996826

Epoch: [12 | 20]
batch_idx: 0  loss: 1.2188363075256348
batch_idx: 250  loss: 1.3223134233765836
batch_idx: 500  loss: 1.3390247538994755
batch_idx: 750  loss: 1.3527178125790043
batch_idx: 1000  loss: 1.3513372119861289
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6738500142097473, top 2 accuracy:0.8246000125408173

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2798231840133667
batch_idx: 250  loss: 1.3643701185482675
batch_idx: 500  loss: 1.3661745420198121
batch_idx: 750  loss: 1.375591133331098
batch_idx: 1000  loss: 1.3723434269095
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6790500111579895, top 2 accuracy:0.8271000127792358

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0806336402893066
batch_idx: 250  loss: 1.3970927659582069
batch_idx: 500  loss: 1.4013627456706106
batch_idx: 750  loss: 1.4078554679511264
batch_idx: 1000  loss: 1.410893646434854
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6917500126361847, top 2 accuracy:0.8318000104427338

Epoch: [15 | 20]
batch_idx: 0  loss: 1.375359058380127
batch_idx: 250  loss: 1.4351595757686086
batch_idx: 500  loss: 1.4345729087718555
batch_idx: 750  loss: 1.4306956016998331
batch_idx: 1000  loss: 1.4315302054198404
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6851000118255616, top 2 accuracy:0.8261250116825104

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3296003341674805
batch_idx: 250  loss: 1.4129800778504207
batch_idx: 500  loss: 1.433275202957637
batch_idx: 750  loss: 1.446938443811984
batch_idx: 1000  loss: 1.4493459838028915
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6737500095367431, top 2 accuracy:0.8151000130176544

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1733407974243164
batch_idx: 250  loss: 1.4485255023064787
batch_idx: 500  loss: 1.459381270161466
batch_idx: 750  loss: 1.4596652973833841
batch_idx: 1000  loss: 1.46632961976452
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6746250116825103, top 2 accuracy:0.8141500113010407

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8111357688903809
batch_idx: 250  loss: 1.5022447169869428
batch_idx: 500  loss: 1.4701537411939205
batch_idx: 750  loss: 1.4743692877446164
batch_idx: 1000  loss: 1.472876612180338
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6659500124454498, top 2 accuracy:0.806125010728836

Epoch: [19 | 20]
batch_idx: 0  loss: 1.1352050304412842
batch_idx: 250  loss: 1.5076364426620434
batch_idx: 500  loss: 1.5036244039710438
batch_idx: 750  loss: 1.5109643493959914
batch_idx: 1000  loss: 1.5131204035906747
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.671700011253357, top 2 accuracy:0.8113000116348267

Epoch: [20 | 20]
batch_idx: 0  loss: 1.968430995941162
batch_idx: 250  loss: 1.5176799064220798
batch_idx: 500  loss: 1.5134449081177537
batch_idx: 750  loss: 1.5245693061756542
batch_idx: 1000  loss: 1.5252222277866765
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6666250133514404, top 2 accuracy:0.8014000127315521
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6917500126361847
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.88335|0.6917500126361847
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.50 train_acc:0.83 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.31 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.26 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.24 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4281930751944574
batch_idx: 500  loss: 1.3151779484710815
batch_idx: 750  loss: 1.2627763829375913
batch_idx: 1000  loss: 1.2294182553054236
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7226750123500824, top 2 accuracy:0.8669500119686127

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9461950659751892
batch_idx: 250  loss: 1.1447999933186699
batch_idx: 500  loss: 1.1580499288948338
batch_idx: 750  loss: 1.1508654129828717
batch_idx: 1000  loss: 1.1576037932270633
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7001250133514404, top 2 accuracy:0.8377750110626221

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4853283166885376
batch_idx: 250  loss: 1.1449536774040974
batch_idx: 500  loss: 1.16356892292009
batch_idx: 750  loss: 1.1705749292142724
batch_idx: 1000  loss: 1.1763168219655467
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6845750122070312, top 2 accuracy:0.8288500130176544

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4350837469100952
batch_idx: 250  loss: 1.196011013749673
batch_idx: 500  loss: 1.1965368818818478
batch_idx: 750  loss: 1.1941203111787626
batch_idx: 1000  loss: 1.1910891824494154
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6806500101089478, top 2 accuracy:0.8298250117301941

Epoch: [5 | 20]
batch_idx: 0  loss: 1.2061729431152344
batch_idx: 250  loss: 1.2127290854203872
batch_idx: 500  loss: 1.2161420306140347
batch_idx: 750  loss: 1.2067404071532266
batch_idx: 1000  loss: 1.2132119012954898
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6786500120162964, top 2 accuracy:0.8286250112056732

Epoch: [6 | 20]
batch_idx: 0  loss: 1.314421534538269
batch_idx: 250  loss: 1.2200391498203307
batch_idx: 500  loss: 1.2266941777827067
batch_idx: 750  loss: 1.224640493890337
batch_idx: 1000  loss: 1.2368224279615825
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6828000135421753, top 2 accuracy:0.8332500100135803

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9772118330001831
batch_idx: 250  loss: 1.2734090720905977
batch_idx: 500  loss: 1.264503497398641
batch_idx: 750  loss: 1.2624485870191808
batch_idx: 1000  loss: 1.2644333731823456
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6785500118732453, top 2 accuracy:0.8318000111579895

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9277406930923462
batch_idx: 250  loss: 1.2710828195505188
batch_idx: 500  loss: 1.27012853360062
batch_idx: 750  loss: 1.2715385176514995
batch_idx: 1000  loss: 1.2771429518779245
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6798500111103057, top 2 accuracy:0.8338250131607056

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3531922101974487
batch_idx: 250  loss: 1.284786207971588
batch_idx: 500  loss: 1.3035882987093887
batch_idx: 750  loss: 1.306963666313947
batch_idx: 1000  loss: 1.3189076887675748
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6775500133037567, top 2 accuracy:0.8344250121116639

Epoch: [10 | 20]
batch_idx: 0  loss: 1.4271224737167358
batch_idx: 250  loss: 1.306401774500436
batch_idx: 500  loss: 1.3227063073781118
batch_idx: 750  loss: 1.323786083690152
batch_idx: 1000  loss: 1.336002711147165
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6757000102996826, top 2 accuracy:0.8308250131607056

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2850042581558228
batch_idx: 250  loss: 1.3467648717291973
batch_idx: 500  loss: 1.3461772203445435
batch_idx: 750  loss: 1.347592724803409
batch_idx: 1000  loss: 1.3487740994071047
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6732750120162964, top 2 accuracy:0.8266250121593476

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1946773529052734
batch_idx: 250  loss: 1.3316675411309271
batch_idx: 500  loss: 1.347417572088409
batch_idx: 750  loss: 1.3616957318725709
batch_idx: 1000  loss: 1.3603114638560878
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6687750105857849, top 2 accuracy:0.8247000112533569

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2851320505142212
batch_idx: 250  loss: 1.3690162511432873
batch_idx: 500  loss: 1.3717379897975466
batch_idx: 750  loss: 1.381579786174281
batch_idx: 1000  loss: 1.378147237669355
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6741250102519989, top 2 accuracy:0.8282000117301941

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0849889516830444
batch_idx: 250  loss: 1.406301255066937
batch_idx: 500  loss: 1.408821535737891
batch_idx: 750  loss: 1.4163424180123703
batch_idx: 1000  loss: 1.4192259205987279
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.685400011062622, top 2 accuracy:0.8314250135421752

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3825963735580444
batch_idx: 250  loss: 1.4423151442659679
batch_idx: 500  loss: 1.4407049272048986
batch_idx: 750  loss: 1.438071031529831
batch_idx: 1000  loss: 1.4387291085939058
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6738750121593475, top 2 accuracy:0.8256500120162964

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3224848508834839
batch_idx: 250  loss: 1.4206819471759524
batch_idx: 500  loss: 1.4402311860659476
batch_idx: 750  loss: 1.4534386371665333
batch_idx: 1000  loss: 1.455665783427013
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6571750113964081, top 2 accuracy:0.809800012588501

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1407654285430908
batch_idx: 250  loss: 1.4465980290987532
batch_idx: 500  loss: 1.4576537282082833
batch_idx: 750  loss: 1.4602990998049377
batch_idx: 1000  loss: 1.4665431167942267
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.665200011730194, top 2 accuracy:0.8085000112056732

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8285421133041382
batch_idx: 250  loss: 1.5075717025417212
batch_idx: 500  loss: 1.476739824388586
batch_idx: 750  loss: 1.480953529770543
batch_idx: 1000  loss: 1.4778599924268052
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.633475013256073, top 2 accuracy:0.793300012588501

Epoch: [19 | 20]
batch_idx: 0  loss: 1.1103845834732056
batch_idx: 250  loss: 1.50786650313482
batch_idx: 500  loss: 1.5064556297312892
batch_idx: 750  loss: 1.5151447819990957
batch_idx: 1000  loss: 1.5168588257397706
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6532500097751618, top 2 accuracy:0.8006750118732452

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9921338558197021
batch_idx: 250  loss: 1.5181297240082903
batch_idx: 500  loss: 1.5163244278997516
batch_idx: 750  loss: 1.526202222589104
batch_idx: 1000  loss: 1.5261535001353335
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6534000115394593, top 2 accuracy:0.7943750131130218
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.685400011062622
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.88105|0.685400011062622
================= iter seed  99  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.67 train_acc:0.75 test_acc:0.80 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.40 train_acc:0.87 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.33 train_acc:0.89 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.29 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.27 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.25 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.24 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.20 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.884125|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4165788943892632
batch_idx: 500  loss: 1.2929336768968633
batch_idx: 750  loss: 1.2369125186603966
batch_idx: 1000  loss: 1.201944921772701
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6962750148773194, top 2 accuracy:0.8477750120162963

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9253606796264648
batch_idx: 250  loss: 1.111395678004703
batch_idx: 500  loss: 1.1245171283705953
batch_idx: 750  loss: 1.1190466146446276
batch_idx: 1000  loss: 1.1248807957330451
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6829750125408173, top 2 accuracy:0.8196750092506409

Epoch: [3 | 20]
batch_idx: 0  loss: 1.5013480186462402
batch_idx: 250  loss: 1.118030450112869
batch_idx: 500  loss: 1.1335997112344898
batch_idx: 750  loss: 1.1423727336432339
batch_idx: 1000  loss: 1.1478548648353584
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6679000103473663, top 2 accuracy:0.811850013256073

Epoch: [4 | 20]
batch_idx: 0  loss: 1.422735571861267
batch_idx: 250  loss: 1.165563276917074
batch_idx: 500  loss: 1.1656450036990396
batch_idx: 750  loss: 1.1633213885988942
batch_idx: 1000  loss: 1.1588587290324723
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6612250125408172, top 2 accuracy:0.8103000123500824

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1500624418258667
batch_idx: 250  loss: 1.183377924329336
batch_idx: 500  loss: 1.1873594290521916
batch_idx: 750  loss: 1.1765834075647064
batch_idx: 1000  loss: 1.1827131419802626
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6574250121116638, top 2 accuracy:0.8092500123977661

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2496870756149292
batch_idx: 250  loss: 1.1882598825592683
batch_idx: 500  loss: 1.1963906343283646
batch_idx: 750  loss: 1.1947347085928903
batch_idx: 1000  loss: 1.2071003598479417
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6649250130653381, top 2 accuracy:0.8135250101089477

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9499483108520508
batch_idx: 250  loss: 1.2456335315454177
batch_idx: 500  loss: 1.236111155774985
batch_idx: 750  loss: 1.2332509757991545
batch_idx: 1000  loss: 1.2357208673327496
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6623500113487244, top 2 accuracy:0.8136250121593476

Epoch: [8 | 20]
batch_idx: 0  loss: 0.8740849494934082
batch_idx: 250  loss: 1.241956859969183
batch_idx: 500  loss: 1.2406214618226559
batch_idx: 750  loss: 1.2424662023735147
batch_idx: 1000  loss: 1.2497992221349346
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6651000115871429, top 2 accuracy:0.8169250111579895

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3000001907348633
batch_idx: 250  loss: 1.2630608643750127
batch_idx: 500  loss: 1.2787893552814373
batch_idx: 750  loss: 1.2805130340559423
batch_idx: 1000  loss: 1.2928021015783848
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6655750126838684, top 2 accuracy:0.817750010728836

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3347549438476562
batch_idx: 250  loss: 1.2841993535266203
batch_idx: 500  loss: 1.2976987507830016
batch_idx: 750  loss: 1.298073644901984
batch_idx: 1000  loss: 1.3102891722235817
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.661675011396408, top 2 accuracy:0.8145000123977661

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2461143732070923
batch_idx: 250  loss: 1.3218117506970284
batch_idx: 500  loss: 1.3222893374293234
batch_idx: 750  loss: 1.3223514200969857
batch_idx: 1000  loss: 1.3230924886731674
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6608250122070313, top 2 accuracy:0.8112250139713287

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1954635381698608
batch_idx: 250  loss: 1.3086524072246823
batch_idx: 500  loss: 1.3244344374874943
batch_idx: 750  loss: 1.3381958261267057
batch_idx: 1000  loss: 1.3359265028478238
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.65660001039505, top 2 accuracy:0.8076750128269196

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2854417562484741
batch_idx: 250  loss: 1.3560112584944939
batch_idx: 500  loss: 1.3556017638963946
batch_idx: 750  loss: 1.3630952093435007
batch_idx: 1000  loss: 1.3578398709003918
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.663750011920929, top 2 accuracy:0.8096250104904175

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0546599626541138
batch_idx: 250  loss: 1.387603629374542
batch_idx: 500  loss: 1.3909750157756289
batch_idx: 750  loss: 1.3964262415970685
batch_idx: 1000  loss: 1.3989401697731627
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6704250123500824, top 2 accuracy:0.8109000115394592

Epoch: [15 | 20]
batch_idx: 0  loss: 1.36911141872406
batch_idx: 250  loss: 1.4222790869885673
batch_idx: 500  loss: 1.421583109637767
batch_idx: 750  loss: 1.4177974416513022
batch_idx: 1000  loss: 1.4189013456717467
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6649000136852264, top 2 accuracy:0.8070250132083893

Epoch: [16 | 20]
batch_idx: 0  loss: 1.34408438205719
batch_idx: 250  loss: 1.4031182347496287
batch_idx: 500  loss: 1.4228405439112175
batch_idx: 750  loss: 1.4353384418078976
batch_idx: 1000  loss: 1.4377396036498844
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6529250130653381, top 2 accuracy:0.7959500117301941

Epoch: [17 | 20]
batch_idx: 0  loss: 1.15476655960083
batch_idx: 250  loss: 1.441264331056507
batch_idx: 500  loss: 1.4544419729918765
batch_idx: 750  loss: 1.4549407426698084
batch_idx: 1000  loss: 1.4611115387072577
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6458000123500824, top 2 accuracy:0.7868250126838684

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8049139976501465
batch_idx: 250  loss: 1.4937377338000057
batch_idx: 500  loss: 1.4644669683165907
batch_idx: 750  loss: 1.4696300728895868
batch_idx: 1000  loss: 1.4691637564486209
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.634525013923645, top 2 accuracy:0.7792500128746033

Epoch: [19 | 20]
batch_idx: 0  loss: 1.1761337518692017
batch_idx: 250  loss: 1.5079817246935894
batch_idx: 500  loss: 1.5076002795160102
batch_idx: 750  loss: 1.5200846480079762
batch_idx: 1000  loss: 1.522356358484719
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6175500106811523, top 2 accuracy:0.7662750113010407

Epoch: [20 | 20]
batch_idx: 0  loss: 2.0130739212036133
batch_idx: 250  loss: 1.5679945206604247
batch_idx: 500  loss: 1.607649927504325
batch_idx: 750  loss: 1.6388391954839199
batch_idx: 1000  loss: 1.6520752279569928
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.48317500710487366, top 2 accuracy:0.7183000133037567
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6704250123500824
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.884125|0.6704250123500824
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.49 train_acc:0.83 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.31 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.26 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.24 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.21 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.12 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.12 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.12 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.12 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.12 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287869215011597
batch_idx: 250  loss: 1.421655220902023
batch_idx: 500  loss: 1.2998275378484285
batch_idx: 750  loss: 1.2440028359688236
batch_idx: 1000  loss: 1.2094531850026438
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.703175011396408, top 2 accuracy:0.8498500115871429

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9222713708877563
batch_idx: 250  loss: 1.122857782912747
batch_idx: 500  loss: 1.137277589840562
batch_idx: 750  loss: 1.1312753732944183
batch_idx: 1000  loss: 1.1369756487802194
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6791250126361847, top 2 accuracy:0.814050010919571

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4681874513626099
batch_idx: 250  loss: 1.131305999520852
batch_idx: 500  loss: 1.1474068341643047
batch_idx: 750  loss: 1.1575119289519495
batch_idx: 1000  loss: 1.1631933561386392
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.660175012588501, top 2 accuracy:0.8006250116825103

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4394304752349854
batch_idx: 250  loss: 1.178980243698781
batch_idx: 500  loss: 1.1791555324905036
batch_idx: 750  loss: 1.178893782218255
batch_idx: 1000  loss: 1.1752068250419232
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6509000129699707, top 2 accuracy:0.7969750108718872

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1926567554473877
batch_idx: 250  loss: 1.2013485405138074
batch_idx: 500  loss: 1.2021357912386053
batch_idx: 750  loss: 1.1914925309430417
batch_idx: 1000  loss: 1.1968829947205397
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6461750121116638, top 2 accuracy:0.7955000112056733

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2520793676376343
batch_idx: 250  loss: 1.201597551473184
batch_idx: 500  loss: 1.2102060984386402
batch_idx: 750  loss: 1.208725975161223
batch_idx: 1000  loss: 1.221309331445077
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.648150013923645, top 2 accuracy:0.7973250133991241

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9491357803344727
batch_idx: 250  loss: 1.2594750175794471
batch_idx: 500  loss: 1.2497939699384395
batch_idx: 750  loss: 1.2475362752205899
batch_idx: 1000  loss: 1.2501578032018277
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6438000116348267, top 2 accuracy:0.7967000136375427

Epoch: [8 | 20]
batch_idx: 0  loss: 0.8992526531219482
batch_idx: 250  loss: 1.2589850149965818
batch_idx: 500  loss: 1.256185933591076
batch_idx: 750  loss: 1.2572460990119576
batch_idx: 1000  loss: 1.2637662492668666
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.642300012588501, top 2 accuracy:0.7962500121593475

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3309071063995361
batch_idx: 250  loss: 1.274549353008998
batch_idx: 500  loss: 1.2916802925071078
batch_idx: 750  loss: 1.2934631556193203
batch_idx: 1000  loss: 1.306254273096022
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6414000120162964, top 2 accuracy:0.7971500132083893

Epoch: [10 | 20]
batch_idx: 0  loss: 1.371513843536377
batch_idx: 250  loss: 1.296210460803088
batch_idx: 500  loss: 1.3108881035584963
batch_idx: 750  loss: 1.3119517584758087
batch_idx: 1000  loss: 1.323805369412937
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6372500140666961, top 2 accuracy:0.792375011920929

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2809147834777832
batch_idx: 250  loss: 1.335709455945723
batch_idx: 500  loss: 1.3339796112389846
batch_idx: 750  loss: 1.3345958882979474
batch_idx: 1000  loss: 1.3360784551063285
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6354000124931335, top 2 accuracy:0.7891000130176544

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1774502992630005
batch_idx: 250  loss: 1.3219075209575162
batch_idx: 500  loss: 1.3374547959799972
batch_idx: 750  loss: 1.3522174572487853
batch_idx: 1000  loss: 1.3500909012632247
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6310500121116638, top 2 accuracy:0.7841750116348266

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2776514291763306
batch_idx: 250  loss: 1.36635885528616
batch_idx: 500  loss: 1.3665849452859096
batch_idx: 750  loss: 1.3732029620544146
batch_idx: 1000  loss: 1.368389124806506
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6354000113010406, top 2 accuracy:0.7857750120162964

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0604095458984375
batch_idx: 250  loss: 1.3949113405755291
batch_idx: 500  loss: 1.3980408270963642
batch_idx: 750  loss: 1.407177223177144
batch_idx: 1000  loss: 1.4100346196764193
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6435500111579895, top 2 accuracy:0.7880250132083892

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3934661149978638
batch_idx: 250  loss: 1.4368509364999913
batch_idx: 500  loss: 1.4323879121973564
batch_idx: 750  loss: 1.4288851368484374
batch_idx: 1000  loss: 1.4304090769003375
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6390250101089477, top 2 accuracy:0.7844000120162964

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3492662906646729
batch_idx: 250  loss: 1.4122867236455787
batch_idx: 500  loss: 1.4307690166305318
batch_idx: 750  loss: 1.4448525082754162
batch_idx: 1000  loss: 1.44740565835287
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6251750137805939, top 2 accuracy:0.7705250132083893

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1806056499481201
batch_idx: 250  loss: 1.4488611245951326
batch_idx: 500  loss: 1.4599193935379078
batch_idx: 750  loss: 1.4595859358449006
batch_idx: 1000  loss: 1.4654519092351104
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6235750107765198, top 2 accuracy:0.7652250127792358

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8270905017852783
batch_idx: 250  loss: 1.505653254367968
batch_idx: 500  loss: 1.4736204300390667
batch_idx: 750  loss: 1.4770544571341067
batch_idx: 1000  loss: 1.4744819875913686
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6087000098228454, top 2 accuracy:0.753425012588501

Epoch: [19 | 20]
batch_idx: 0  loss: 1.1450116634368896
batch_idx: 250  loss: 1.5082916398495672
batch_idx: 500  loss: 1.5048511264997235
batch_idx: 750  loss: 1.5143839736261413
batch_idx: 1000  loss: 1.5174703248107015
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6145500121116638, top 2 accuracy:0.7571250109672546

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9930672645568848
batch_idx: 250  loss: 1.533319826345944
batch_idx: 500  loss: 1.5385209745007078
batch_idx: 750  loss: 1.5731543050667527
batch_idx: 1000  loss: 1.606495650264973
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5333500115871429, top 2 accuracy:0.7113750126361847
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6438000116348267
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.882375|0.6438000116348267
================= iter seed  100  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.66 train_acc:0.75 test_acc:0.80 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.41 train_acc:0.88 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.34 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.30 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.27 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.26 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.24 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.21 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.11 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.11 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.11 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.11 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.884025|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4147756076958298
batch_idx: 500  loss: 1.2932973315841274
batch_idx: 750  loss: 1.2360371001988666
batch_idx: 1000  loss: 1.2000607809724329
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7483000130653381, top 2 accuracy:0.8693500113487244

Epoch: [2 | 20]
batch_idx: 0  loss: 0.8845018744468689
batch_idx: 250  loss: 1.097953899297123
batch_idx: 500  loss: 1.11181233355113
batch_idx: 750  loss: 1.1050005354673194
batch_idx: 1000  loss: 1.1106118101376694
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6914750139713287, top 2 accuracy:0.8274500112533569

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4788119792938232
batch_idx: 250  loss: 1.100212428641812
batch_idx: 500  loss: 1.1180295018298014
batch_idx: 750  loss: 1.1247638454140343
batch_idx: 1000  loss: 1.1304228812122878
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6699500117301941, top 2 accuracy:0.812950011253357

Epoch: [4 | 20]
batch_idx: 0  loss: 1.3987009525299072
batch_idx: 250  loss: 1.1475447382191595
batch_idx: 500  loss: 1.1475163930626007
batch_idx: 750  loss: 1.144313000027584
batch_idx: 1000  loss: 1.14047687757796
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6626250116825104, top 2 accuracy:0.8110250120162964

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1072592735290527
batch_idx: 250  loss: 1.1633656482628305
batch_idx: 500  loss: 1.168137512233649
batch_idx: 750  loss: 1.1581470997426153
batch_idx: 1000  loss: 1.1637810420590087
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6596750121116638, top 2 accuracy:0.8116250107288361

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2870912551879883
batch_idx: 250  loss: 1.168024163177927
batch_idx: 500  loss: 1.1783673985723102
batch_idx: 750  loss: 1.1759506034813008
batch_idx: 1000  loss: 1.1877270890548588
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6623500127792359, top 2 accuracy:0.8146000120639801

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9181917905807495
batch_idx: 250  loss: 1.2281315283100631
batch_idx: 500  loss: 1.2165991946270591
batch_idx: 750  loss: 1.21249403973854
batch_idx: 1000  loss: 1.2148917265736257
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6571500108242035, top 2 accuracy:0.8134000115394592

Epoch: [8 | 20]
batch_idx: 0  loss: 0.8528555631637573
batch_idx: 250  loss: 1.2218555880085653
batch_idx: 500  loss: 1.2211546777157882
batch_idx: 750  loss: 1.2214014297346285
batch_idx: 1000  loss: 1.2278847827698096
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6601250114440917, top 2 accuracy:0.8150250129699707

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3151277303695679
batch_idx: 250  loss: 1.239091691796465
batch_idx: 500  loss: 1.2587304210358639
batch_idx: 750  loss: 1.2603245660813327
batch_idx: 1000  loss: 1.2728965784223696
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6552000114917755, top 2 accuracy:0.8155000112056732

Epoch: [10 | 20]
batch_idx: 0  loss: 1.2882825136184692
batch_idx: 250  loss: 1.255288191933321
batch_idx: 500  loss: 1.271063561122003
batch_idx: 750  loss: 1.27381382018978
batch_idx: 1000  loss: 1.2865767873133334
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6527000141143798, top 2 accuracy:0.8125250115394592

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2873154878616333
batch_idx: 250  loss: 1.3006006745541607
batch_idx: 500  loss: 1.3002490067120753
batch_idx: 750  loss: 1.3000864764044042
batch_idx: 1000  loss: 1.301488054350923
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.650775013923645, top 2 accuracy:0.8098500125408172

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1603806018829346
batch_idx: 250  loss: 1.287395772191034
batch_idx: 500  loss: 1.30129969248361
batch_idx: 750  loss: 1.3158049342034661
batch_idx: 1000  loss: 1.313295203252151
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6493000109195709, top 2 accuracy:0.8080500118732452

Epoch: [13 | 20]
batch_idx: 0  loss: 1.271262288093567
batch_idx: 250  loss: 1.3269383939536843
batch_idx: 500  loss: 1.3280290276334996
batch_idx: 750  loss: 1.3357287182523698
batch_idx: 1000  loss: 1.3313051162722012
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6531000118255615, top 2 accuracy:0.8112500133514404

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0154483318328857
batch_idx: 250  loss: 1.3552127902193176
batch_idx: 500  loss: 1.358756022970452
batch_idx: 750  loss: 1.3675554621530506
batch_idx: 1000  loss: 1.3706335549632582
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6646000130176544, top 2 accuracy:0.8158250117301941

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3507578372955322
batch_idx: 250  loss: 1.3978313342186908
batch_idx: 500  loss: 1.397133191806848
batch_idx: 750  loss: 1.3920820001720937
batch_idx: 1000  loss: 1.3917334017852625
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6538250131607055, top 2 accuracy:0.8111250131130219

Epoch: [16 | 20]
batch_idx: 0  loss: 1.303985834121704
batch_idx: 250  loss: 1.3767507868465065
batch_idx: 500  loss: 1.39627591959027
batch_idx: 750  loss: 1.4074786675934337
batch_idx: 1000  loss: 1.4084785667518838
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6354250116348267, top 2 accuracy:0.798300012588501

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1251239776611328
batch_idx: 250  loss: 1.402874939771449
batch_idx: 500  loss: 1.4142794448412968
batch_idx: 750  loss: 1.4154370365274784
batch_idx: 1000  loss: 1.4218007735789013
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6489750127792359, top 2 accuracy:0.8016750109195709

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8681689500808716
batch_idx: 250  loss: 1.458178007545835
batch_idx: 500  loss: 1.4291707539672487
batch_idx: 750  loss: 1.4358237374012872
batch_idx: 1000  loss: 1.4353684735374328
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.599900012254715, top 2 accuracy:0.7853500125408173

Epoch: [19 | 20]
batch_idx: 0  loss: 1.0250914096832275
batch_idx: 250  loss: 1.457069808424751
batch_idx: 500  loss: 1.4538228205706705
batch_idx: 750  loss: 1.4640739259090494
batch_idx: 1000  loss: 1.4679238616277615
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6159000124931335, top 2 accuracy:0.7917250134944915

Epoch: [20 | 20]
batch_idx: 0  loss: 1.8766974210739136
batch_idx: 250  loss: 1.4687295820451503
batch_idx: 500  loss: 1.467843819082829
batch_idx: 750  loss: 1.4744784197571303
batch_idx: 1000  loss: 1.4719727841048196
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6380000112056732, top 2 accuracy:0.7934750127792358
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6646000130176544
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.884025|0.6646000130176544
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.50 train_acc:0.83 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.31 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.26 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.24 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.21 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.21 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287869215011597
batch_idx: 250  loss: 1.4255781484522008
batch_idx: 500  loss: 1.3090028268963907
batch_idx: 750  loss: 1.2538235957410628
batch_idx: 1000  loss: 1.2189552391917942
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7422250111103058, top 2 accuracy:0.8700250120162963

Epoch: [2 | 20]
batch_idx: 0  loss: 0.8912508487701416
batch_idx: 250  loss: 1.1254299030015882
batch_idx: 500  loss: 1.1389807256405053
batch_idx: 750  loss: 1.1320835009669292
batch_idx: 1000  loss: 1.137856529948239
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6880750105381012, top 2 accuracy:0.8249250121116638

Epoch: [3 | 20]
batch_idx: 0  loss: 1.5001580715179443
batch_idx: 250  loss: 1.1265306859403044
batch_idx: 500  loss: 1.1456317328855372
batch_idx: 750  loss: 1.1528328012887183
batch_idx: 1000  loss: 1.1588809733716443
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6625000104904175, top 2 accuracy:0.8054250102043152

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4212795495986938
batch_idx: 250  loss: 1.1753341580233247
batch_idx: 500  loss: 1.17538810831508
batch_idx: 750  loss: 1.1732868834529042
batch_idx: 1000  loss: 1.1701113960137381
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.654575012922287, top 2 accuracy:0.802400011062622

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1623250246047974
batch_idx: 250  loss: 1.1922367565772112
batch_idx: 500  loss: 1.1956520031133526
batch_idx: 750  loss: 1.186279321110204
batch_idx: 1000  loss: 1.1912409334946363
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.650750011920929, top 2 accuracy:0.8014250128269196

Epoch: [6 | 20]
batch_idx: 0  loss: 1.3370693922042847
batch_idx: 250  loss: 1.195079748884483
batch_idx: 500  loss: 1.205337373738844
batch_idx: 750  loss: 1.2043482337053815
batch_idx: 1000  loss: 1.2162811450493602
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.654725013256073, top 2 accuracy:0.8072750113010406

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9610218405723572
batch_idx: 250  loss: 1.2555533953615137
batch_idx: 500  loss: 1.2440545869310886
batch_idx: 750  loss: 1.2409500129493136
batch_idx: 1000  loss: 1.2431476303516105
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6484500114917755, top 2 accuracy:0.8063750112056732

Epoch: [8 | 20]
batch_idx: 0  loss: 0.884623646736145
batch_idx: 250  loss: 1.253977966138024
batch_idx: 500  loss: 1.2509957924604036
batch_idx: 750  loss: 1.2513962253856812
batch_idx: 1000  loss: 1.2569274493394949
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6502500095367432, top 2 accuracy:0.8077250115871429

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3558802604675293
batch_idx: 250  loss: 1.2659628495502926
batch_idx: 500  loss: 1.2852373041889884
batch_idx: 750  loss: 1.2881862157009079
batch_idx: 1000  loss: 1.3013178902312208
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6464750106334687, top 2 accuracy:0.8068750126361847

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3458967208862305
batch_idx: 250  loss: 1.2829360353548689
batch_idx: 500  loss: 1.297768717985214
batch_idx: 750  loss: 1.2995676403669933
batch_idx: 1000  loss: 1.3122344946804139
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6438500099182128, top 2 accuracy:0.8049000120162964

Epoch: [11 | 20]
batch_idx: 0  loss: 1.3031251430511475
batch_idx: 250  loss: 1.327221635225657
batch_idx: 500  loss: 1.3263382651208881
batch_idx: 750  loss: 1.3269964845780682
batch_idx: 1000  loss: 1.329386606622047
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6445000116825104, top 2 accuracy:0.8035750124454498

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1750973463058472
batch_idx: 250  loss: 1.313343579894219
batch_idx: 500  loss: 1.32859628071245
batch_idx: 750  loss: 1.3436613351249391
batch_idx: 1000  loss: 1.3408229215124163
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6418250114917755, top 2 accuracy:0.8034000117778778

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2725040912628174
batch_idx: 250  loss: 1.3503415178039684
batch_idx: 500  loss: 1.3520516473235507
batch_idx: 750  loss: 1.3598536849656342
batch_idx: 1000  loss: 1.3555853632977977
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6405250093936921, top 2 accuracy:0.804100011587143

Epoch: [14 | 20]
batch_idx: 0  loss: 1.049943208694458
batch_idx: 250  loss: 1.3786661560470994
batch_idx: 500  loss: 1.3813765882209919
batch_idx: 750  loss: 1.3907017654215421
batch_idx: 1000  loss: 1.3938210604908747
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6513250102996826, top 2 accuracy:0.8114250123500824

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3617132902145386
batch_idx: 250  loss: 1.4259413404949899
batch_idx: 500  loss: 1.4247683213468183
batch_idx: 750  loss: 1.4196887305596206
batch_idx: 1000  loss: 1.4187294705607258
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6368750109672546, top 2 accuracy:0.8042250108718872

Epoch: [16 | 20]
batch_idx: 0  loss: 1.336502194404602
batch_idx: 250  loss: 1.3935091036870861
batch_idx: 500  loss: 1.4173871559199345
batch_idx: 750  loss: 1.4306872611099128
batch_idx: 1000  loss: 1.430761895573939
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6158750121593475, top 2 accuracy:0.7889000108242035

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1634100675582886
batch_idx: 250  loss: 1.418142537433885
batch_idx: 500  loss: 1.4300427134527545
batch_idx: 750  loss: 1.4332534683201146
batch_idx: 1000  loss: 1.4403956715767376
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6242500116825104, top 2 accuracy:0.7906000106334686

Epoch: [18 | 20]
batch_idx: 0  loss: 1.9218090772628784
batch_idx: 250  loss: 1.4797428052262396
batch_idx: 500  loss: 1.453920493380685
batch_idx: 750  loss: 1.4707955694883792
batch_idx: 1000  loss: 1.4802956638482814
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5077750089168549, top 2 accuracy:0.7659000122547149

Epoch: [19 | 20]
batch_idx: 0  loss: 1.0585966110229492
batch_idx: 250  loss: 1.543525080896902
batch_idx: 500  loss: 1.5393642896765538
batch_idx: 750  loss: 1.5535264968427969
batch_idx: 1000  loss: 1.561592890312687
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.3776500052213669, top 2 accuracy:0.7555250136852264

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9046586751937866
batch_idx: 250  loss: 1.5799659596717603
batch_idx: 500  loss: 1.5870388068460772
batch_idx: 750  loss: 1.5988062895505335
batch_idx: 1000  loss: 1.6016480265238795
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.3079250057339668, top 2 accuracy:0.7511000106334687
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6513250102996826
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.880575|0.6513250102996826
================= iter seed  101  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.63 train_acc:0.76 test_acc:0.80 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.38 train_acc:0.89 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.31 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.28 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.26 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.24 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.21 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.20 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.20 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.88435|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287869215011597
batch_idx: 250  loss: 1.429362301132827
batch_idx: 500  loss: 1.3179602748469303
batch_idx: 750  loss: 1.2665607936842889
batch_idx: 1000  loss: 1.232368604800762
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7229000113010406, top 2 accuracy:0.8655750119686126

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9374356269836426
batch_idx: 250  loss: 1.1397313000856408
batch_idx: 500  loss: 1.153905907553729
batch_idx: 750  loss: 1.1473594487760217
batch_idx: 1000  loss: 1.1537602554304531
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7045500135421753, top 2 accuracy:0.8233750102519989

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4720163345336914
batch_idx: 250  loss: 1.1467667479772825
batch_idx: 500  loss: 1.1633639501992976
batch_idx: 750  loss: 1.170640347199341
batch_idx: 1000  loss: 1.1757180155894627
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6841250128746033, top 2 accuracy:0.8116750123500824

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4304500818252563
batch_idx: 250  loss: 1.1916153047156828
batch_idx: 500  loss: 1.1938313075610134
batch_idx: 750  loss: 1.1922117386046973
batch_idx: 1000  loss: 1.1893730129772864
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6751750113964081, top 2 accuracy:0.8066500115394593

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1647062301635742
batch_idx: 250  loss: 1.2119176831268166
batch_idx: 500  loss: 1.2173774055601878
batch_idx: 750  loss: 1.20672271646928
batch_idx: 1000  loss: 1.2128065202992184
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6734000115394593, top 2 accuracy:0.8061500110626221

Epoch: [6 | 20]
batch_idx: 0  loss: 1.3205668926239014
batch_idx: 250  loss: 1.2200347432460996
batch_idx: 500  loss: 1.2267740331293102
batch_idx: 750  loss: 1.225186805289879
batch_idx: 1000  loss: 1.2372714753825063
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6783250153064728, top 2 accuracy:0.812675012588501

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9905346632003784
batch_idx: 250  loss: 1.273216091676812
batch_idx: 500  loss: 1.2656659783834094
batch_idx: 750  loss: 1.263280142965058
batch_idx: 1000  loss: 1.2660132653939837
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6765750131607056, top 2 accuracy:0.8102000102996826

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9322015047073364
batch_idx: 250  loss: 1.2718652115338178
batch_idx: 500  loss: 1.273123253713194
batch_idx: 750  loss: 1.2742603116050688
batch_idx: 1000  loss: 1.2805442712701167
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6788000137805938, top 2 accuracy:0.8116750121116638

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3913553953170776
batch_idx: 250  loss: 1.2854882836910424
batch_idx: 500  loss: 1.304466783477549
batch_idx: 750  loss: 1.3069622678411583
batch_idx: 1000  loss: 1.319383002365359
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6773000118732453, top 2 accuracy:0.811925012588501

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3332122564315796
batch_idx: 250  loss: 1.3142125826382296
batch_idx: 500  loss: 1.326479030282874
batch_idx: 750  loss: 1.3260366421167111
batch_idx: 1000  loss: 1.3372623196806008
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6725750105381012, top 2 accuracy:0.8083000125885009

Epoch: [11 | 20]
batch_idx: 0  loss: 1.3831433057785034
batch_idx: 250  loss: 1.349840285762884
batch_idx: 500  loss: 1.3494186747872658
batch_idx: 750  loss: 1.3499768377936476
batch_idx: 1000  loss: 1.3506120571884483
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6753500113487244, top 2 accuracy:0.8076500129699707

Epoch: [12 | 20]
batch_idx: 0  loss: 1.2311619520187378
batch_idx: 250  loss: 1.3350266412445395
batch_idx: 500  loss: 1.351104054962429
batch_idx: 750  loss: 1.3653241006093433
batch_idx: 1000  loss: 1.363441792040016
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.668600011587143, top 2 accuracy:0.8026250126361847

Epoch: [13 | 20]
batch_idx: 0  loss: 1.319633960723877
batch_idx: 250  loss: 1.3754746347239362
batch_idx: 500  loss: 1.3763424841125615
batch_idx: 750  loss: 1.3854621032440262
batch_idx: 1000  loss: 1.3807198043924551
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6705500109195709, top 2 accuracy:0.80285001039505

Epoch: [14 | 20]
batch_idx: 0  loss: 1.097251057624817
batch_idx: 250  loss: 1.4039828436173771
batch_idx: 500  loss: 1.4071161555901668
batch_idx: 750  loss: 1.4159598613178686
batch_idx: 1000  loss: 1.4190835524290895
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6806750113964081, top 2 accuracy:0.8099000120162964

Epoch: [15 | 20]
batch_idx: 0  loss: 1.4023948907852173
batch_idx: 250  loss: 1.4439400180534643
batch_idx: 500  loss: 1.4420097460777186
batch_idx: 750  loss: 1.4370843512127538
batch_idx: 1000  loss: 1.4376507689492009
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6722500104904174, top 2 accuracy:0.8056000115871429

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3486535549163818
batch_idx: 250  loss: 1.4249115231874827
batch_idx: 500  loss: 1.4415589128479813
batch_idx: 750  loss: 1.4561862225796454
batch_idx: 1000  loss: 1.4584395832861194
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6597500121593476, top 2 accuracy:0.793550012588501

Epoch: [17 | 20]
batch_idx: 0  loss: 1.2027859687805176
batch_idx: 250  loss: 1.4634192761252909
batch_idx: 500  loss: 1.471761082062881
batch_idx: 750  loss: 1.4712685283160452
batch_idx: 1000  loss: 1.4780399378496236
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6618500130176544, top 2 accuracy:0.7944500119686126

Epoch: [18 | 20]
batch_idx: 0  loss: 1.802750825881958
batch_idx: 250  loss: 1.5072013816317997
batch_idx: 500  loss: 1.4781767226862566
batch_idx: 750  loss: 1.4854171995535501
batch_idx: 1000  loss: 1.4836116747353405
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6455250113010407, top 2 accuracy:0.78235001039505

Epoch: [19 | 20]
batch_idx: 0  loss: 1.1469417810440063
batch_idx: 250  loss: 1.5111802729210906
batch_idx: 500  loss: 1.5083846900451696
batch_idx: 750  loss: 1.518015810947205
batch_idx: 1000  loss: 1.5206882772735133
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6528250107765198, top 2 accuracy:0.7856750099658966

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9878692626953125
batch_idx: 250  loss: 1.5313632878665895
batch_idx: 500  loss: 1.5244382414521214
batch_idx: 750  loss: 1.532899520129457
batch_idx: 1000  loss: 1.5311049890880006
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6571000108718872, top 2 accuracy:0.7844500122070313
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6806750113964081
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.88435|0.6806750113964081
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.47 train_acc:0.84 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.29 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.25 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.23 train_acc:0.94 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.18 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287869215011597
batch_idx: 250  loss: 1.4322994498266515
batch_idx: 500  loss: 1.3253151790567181
batch_idx: 750  loss: 1.2754757105828347
batch_idx: 1000  loss: 1.2418637336633456
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6999750130176544, top 2 accuracy:0.8541250128746033

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9141094088554382
batch_idx: 250  loss: 1.159017316771994
batch_idx: 500  loss: 1.1716176656919994
batch_idx: 750  loss: 1.1646160633402851
batch_idx: 1000  loss: 1.1713296900065944
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6823750112056732, top 2 accuracy:0.8109750120639801

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4745516777038574
batch_idx: 250  loss: 1.1615010051734875
batch_idx: 500  loss: 1.178135389583913
batch_idx: 750  loss: 1.1864678976762923
batch_idx: 1000  loss: 1.1921085271115501
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6578250131607055, top 2 accuracy:0.7984750118255616

Epoch: [4 | 20]
batch_idx: 0  loss: 1.457835078239441
batch_idx: 250  loss: 1.2089718068536779
batch_idx: 500  loss: 1.2098173381228956
batch_idx: 750  loss: 1.2078113651389832
batch_idx: 1000  loss: 1.205093474529041
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6529250102043151, top 2 accuracy:0.7982500128746033

Epoch: [5 | 20]
batch_idx: 0  loss: 1.2223594188690186
batch_idx: 250  loss: 1.2254737748250673
batch_idx: 500  loss: 1.2308171644830628
batch_idx: 750  loss: 1.2207165618029092
batch_idx: 1000  loss: 1.2273423076628116
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6516000106334686, top 2 accuracy:0.7980250115394593

Epoch: [6 | 20]
batch_idx: 0  loss: 1.3596142530441284
batch_idx: 250  loss: 1.2337843631712546
batch_idx: 500  loss: 1.240252011605617
batch_idx: 750  loss: 1.2392982897002245
batch_idx: 1000  loss: 1.2523577653419096
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6568750116825104, top 2 accuracy:0.8032500095367432

Epoch: [7 | 20]
batch_idx: 0  loss: 1.0070881843566895
batch_idx: 250  loss: 1.2915694972289955
batch_idx: 500  loss: 1.2814562844032307
batch_idx: 750  loss: 1.278415659154077
batch_idx: 1000  loss: 1.2808589846276628
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6532750113010406, top 2 accuracy:0.8025750131607056

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9519848227500916
batch_idx: 250  loss: 1.2877837783391979
batch_idx: 500  loss: 1.2872315488363568
batch_idx: 750  loss: 1.2879233099857248
batch_idx: 1000  loss: 1.293752153555806
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6535750114917755, top 2 accuracy:0.8034500126838684

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3935446739196777
batch_idx: 250  loss: 1.301001738099491
batch_idx: 500  loss: 1.3208499543689656
batch_idx: 750  loss: 1.3236810840337183
batch_idx: 1000  loss: 1.3358364683656265
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6549250104427338, top 2 accuracy:0.8044250130653381

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3554883003234863
batch_idx: 250  loss: 1.3281579228008495
batch_idx: 500  loss: 1.339144601158358
batch_idx: 750  loss: 1.3393126830514153
batch_idx: 1000  loss: 1.3518780509646708
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6490500123500824, top 2 accuracy:0.8001500129699707

Epoch: [11 | 20]
batch_idx: 0  loss: 1.368994951248169
batch_idx: 250  loss: 1.3629849950156645
batch_idx: 500  loss: 1.3626563094639512
batch_idx: 750  loss: 1.3642353719380893
batch_idx: 1000  loss: 1.3651523356334851
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.650225013256073, top 2 accuracy:0.7966750109195709

Epoch: [12 | 20]
batch_idx: 0  loss: 1.2090407609939575
batch_idx: 250  loss: 1.3511755974379935
batch_idx: 500  loss: 1.366002149083778
batch_idx: 750  loss: 1.3797414708226332
batch_idx: 1000  loss: 1.3775037057197894
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6461750130653381, top 2 accuracy:0.7937250118255615

Epoch: [13 | 20]
batch_idx: 0  loss: 1.3005316257476807
batch_idx: 250  loss: 1.3912467776497142
batch_idx: 500  loss: 1.3925696115649677
batch_idx: 750  loss: 1.4002963699134758
batch_idx: 1000  loss: 1.3957328282939359
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6514250121116638, top 2 accuracy:0.7973000123500824

Epoch: [14 | 20]
batch_idx: 0  loss: 1.1044288873672485
batch_idx: 250  loss: 1.4187713797028
batch_idx: 500  loss: 1.4213502158958946
batch_idx: 750  loss: 1.4301691085750463
batch_idx: 1000  loss: 1.432851027661619
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.661625013589859, top 2 accuracy:0.8058750131130219

Epoch: [15 | 20]
batch_idx: 0  loss: 1.4515104293823242
batch_idx: 250  loss: 1.457243426615559
batch_idx: 500  loss: 1.454064932166104
batch_idx: 750  loss: 1.4496936678822971
batch_idx: 1000  loss: 1.4509205969092183
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6529500129222869, top 2 accuracy:0.7991500120162964

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3563565015792847
batch_idx: 250  loss: 1.4370062161704884
batch_idx: 500  loss: 1.4543037259597718
batch_idx: 750  loss: 1.4689081815914502
batch_idx: 1000  loss: 1.47069828816877
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6410750107765197, top 2 accuracy:0.7852000122070313

Epoch: [17 | 20]
batch_idx: 0  loss: 1.2223738431930542
batch_idx: 250  loss: 1.4735218790642597
batch_idx: 500  loss: 1.4826356772410623
batch_idx: 750  loss: 1.4815316282097744
batch_idx: 1000  loss: 1.4883933694789204
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6478000135421753, top 2 accuracy:0.785375011920929

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8070995807647705
batch_idx: 250  loss: 1.5217042145554704
batch_idx: 500  loss: 1.4928067756231511
batch_idx: 750  loss: 1.4981411301499672
batch_idx: 1000  loss: 1.495505441301547
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6127000136375427, top 2 accuracy:0.7674750103950501

Epoch: [19 | 20]
batch_idx: 0  loss: 1.1249446868896484
batch_idx: 250  loss: 1.5197222528472802
batch_idx: 500  loss: 1.5198207885454715
batch_idx: 750  loss: 1.529402737417013
batch_idx: 1000  loss: 1.5318340254477418
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6225250113010407, top 2 accuracy:0.7700250110626221

Epoch: [20 | 20]
batch_idx: 0  loss: 1.966008186340332
batch_idx: 250  loss: 1.5399105882417228
batch_idx: 500  loss: 1.5335170065767267
batch_idx: 750  loss: 1.541984432488442
batch_idx: 1000  loss: 1.5408548898399828
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6322750140428544, top 2 accuracy:0.7690750110149384
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.661625013589859
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.88205|0.661625013589859
================= iter seed  0  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.60 train_acc:0.78 test_acc:0.81 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.37 train_acc:0.89 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.31 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.27 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.25 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.24 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.88285|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287869215011597
batch_idx: 250  loss: 1.424145359966448
batch_idx: 500  loss: 1.3065566722856183
batch_idx: 750  loss: 1.250907225929847
batch_idx: 1000  loss: 1.2159252415497463
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7232000095844269, top 2 accuracy:0.8645500113964081

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9050040245056152
batch_idx: 250  loss: 1.1189868890803265
batch_idx: 500  loss: 1.1316528018486747
batch_idx: 750  loss: 1.1263155132421094
batch_idx: 1000  loss: 1.1318581171643238
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7006500124931335, top 2 accuracy:0.8303000130653382

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4629735946655273
batch_idx: 250  loss: 1.122990022876873
batch_idx: 500  loss: 1.1396882443717031
batch_idx: 750  loss: 1.1461529003742719
batch_idx: 1000  loss: 1.1508361889531438
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6814250104427337, top 2 accuracy:0.8241500103473663

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4059948921203613
batch_idx: 250  loss: 1.1630994277652382
batch_idx: 500  loss: 1.1661552662769574
batch_idx: 750  loss: 1.1649573634062884
batch_idx: 1000  loss: 1.1626851287798379
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6776250131130218, top 2 accuracy:0.822850011587143

Epoch: [5 | 20]
batch_idx: 0  loss: 1.141916275024414
batch_idx: 250  loss: 1.186757323188509
batch_idx: 500  loss: 1.18919635679353
batch_idx: 750  loss: 1.178737923590156
batch_idx: 1000  loss: 1.1848061617713768
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6784250118732452, top 2 accuracy:0.8245250132083893

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2492527961730957
batch_idx: 250  loss: 1.1893700744086115
batch_idx: 500  loss: 1.1972548830927463
batch_idx: 750  loss: 1.195128772092538
batch_idx: 1000  loss: 1.2084058745981405
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.684800012588501, top 2 accuracy:0.8302250113487244

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9550420045852661
batch_idx: 250  loss: 1.2475334068550026
batch_idx: 500  loss: 1.238169325643369
batch_idx: 750  loss: 1.2341090044231728
batch_idx: 1000  loss: 1.2362765299912077
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6816750121116638, top 2 accuracy:0.8287750091552735

Epoch: [8 | 20]
batch_idx: 0  loss: 0.8910910487174988
batch_idx: 250  loss: 1.2431753989433447
batch_idx: 500  loss: 1.2428744415822401
batch_idx: 750  loss: 1.2429044803385407
batch_idx: 1000  loss: 1.2488300245457564
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6845250120162963, top 2 accuracy:0.8341250119209289

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3132013082504272
batch_idx: 250  loss: 1.258086203770721
batch_idx: 500  loss: 1.2756500901027539
batch_idx: 750  loss: 1.2771178761525888
batch_idx: 1000  loss: 1.290168345164948
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6850500113964081, top 2 accuracy:0.8363500125408173

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3338429927825928
batch_idx: 250  loss: 1.276078818996684
batch_idx: 500  loss: 1.294829093620918
batch_idx: 750  loss: 1.2950323272284325
batch_idx: 1000  loss: 1.3073857303577872
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6843750109672546, top 2 accuracy:0.8360750124454498

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2791717052459717
batch_idx: 250  loss: 1.3152656992971423
batch_idx: 500  loss: 1.3169594485033451
batch_idx: 750  loss: 1.3171956490937415
batch_idx: 1000  loss: 1.3190253809951364
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6835500109195709, top 2 accuracy:0.8349500117301941

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1857855319976807
batch_idx: 250  loss: 1.3055994789247862
batch_idx: 500  loss: 1.3201480495967743
batch_idx: 750  loss: 1.3342013012354141
batch_idx: 1000  loss: 1.332544464582262
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6797500123977661, top 2 accuracy:0.8335500102043152

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2862534523010254
batch_idx: 250  loss: 1.346464479765566
batch_idx: 500  loss: 1.3475933377727558
batch_idx: 750  loss: 1.3559488213842634
batch_idx: 1000  loss: 1.3522400853637688
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6845000100135803, top 2 accuracy:0.8354500131607056

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0765286684036255
batch_idx: 250  loss: 1.3739424225635861
batch_idx: 500  loss: 1.37865870078785
batch_idx: 750  loss: 1.3852990798457878
batch_idx: 1000  loss: 1.3889272002556834
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.690400011062622, top 2 accuracy:0.8373000113964081

Epoch: [15 | 20]
batch_idx: 0  loss: 1.359175682067871
batch_idx: 250  loss: 1.4185218409248679
batch_idx: 500  loss: 1.416838274998338
batch_idx: 750  loss: 1.4116594469122203
batch_idx: 1000  loss: 1.410757035064621
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6879250130653382, top 2 accuracy:0.835325012922287

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3306018114089966
batch_idx: 250  loss: 1.3899509480344472
batch_idx: 500  loss: 1.4132484977135058
batch_idx: 750  loss: 1.4252179043923623
batch_idx: 1000  loss: 1.425725995161282
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6756750125885009, top 2 accuracy:0.8286000120639802

Epoch: [17 | 20]
batch_idx: 0  loss: 1.154916524887085
batch_idx: 250  loss: 1.4216505745825594
batch_idx: 500  loss: 1.4337795442371277
batch_idx: 750  loss: 1.4349661699440202
batch_idx: 1000  loss: 1.4412384491901808
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.681475011587143, top 2 accuracy:0.8295500109195709

Epoch: [18 | 20]
batch_idx: 0  loss: 1.7497897148132324
batch_idx: 250  loss: 1.4785004257208214
batch_idx: 500  loss: 1.44655833670207
batch_idx: 750  loss: 1.4507376153523646
batch_idx: 1000  loss: 1.4496851444434815
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6608750138282776, top 2 accuracy:0.8199250106811523

Epoch: [19 | 20]
batch_idx: 0  loss: 1.0863263607025146
batch_idx: 250  loss: 1.4775371221745526
batch_idx: 500  loss: 1.474087158838908
batch_idx: 750  loss: 1.483796709265209
batch_idx: 1000  loss: 1.486740181859309
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.681750013589859, top 2 accuracy:0.8315000104904174

Epoch: [20 | 20]
batch_idx: 0  loss: 1.935143232345581
batch_idx: 250  loss: 1.4912590950206277
batch_idx: 500  loss: 1.4875141115089734
batch_idx: 750  loss: 1.4985797699618175
batch_idx: 1000  loss: 1.499471907846082
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6827750117778778, top 2 accuracy:0.8208500120639801
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.690400011062622
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.88285|0.690400011062622
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.45 train_acc:0.84 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.29 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.24 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287869215011597
batch_idx: 250  loss: 1.4284996903947125
batch_idx: 500  loss: 1.312757261251909
batch_idx: 750  loss: 1.2565468057411158
batch_idx: 1000  loss: 1.2209617308677194
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7014750154018402, top 2 accuracy:0.8345250136852265

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9001082181930542
batch_idx: 250  loss: 1.1298127659553565
batch_idx: 500  loss: 1.142796272438679
batch_idx: 750  loss: 1.1341276759159824
batch_idx: 1000  loss: 1.1390119196174624
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6679000117778778, top 2 accuracy:0.8154500126838684

Epoch: [3 | 20]
batch_idx: 0  loss: 1.484443187713623
batch_idx: 250  loss: 1.1258076889905337
batch_idx: 500  loss: 1.1439035167439322
batch_idx: 750  loss: 1.149888806834127
batch_idx: 1000  loss: 1.1553376515547689
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6444750111103058, top 2 accuracy:0.8048750114440918

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4415452480316162
batch_idx: 250  loss: 1.1708395296325744
batch_idx: 500  loss: 1.174348566092943
batch_idx: 750  loss: 1.1716955177259927
batch_idx: 1000  loss: 1.1687452749798473
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6395250117778778, top 2 accuracy:0.8048250105381012

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1744812726974487
batch_idx: 250  loss: 1.187137642137955
batch_idx: 500  loss: 1.1926646271105588
batch_idx: 750  loss: 1.183833392901012
batch_idx: 1000  loss: 1.1890619388355996
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6387750127315521, top 2 accuracy:0.8049750120639801

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2760251760482788
batch_idx: 250  loss: 1.1915996929420387
batch_idx: 500  loss: 1.2031277234189248
batch_idx: 750  loss: 1.2016428419413117
batch_idx: 1000  loss: 1.2148849377616906
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6487500131130218, top 2 accuracy:0.8157000112533569

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9475211501121521
batch_idx: 250  loss: 1.2540657367729042
batch_idx: 500  loss: 1.2416928956668343
batch_idx: 750  loss: 1.2387916117097673
batch_idx: 1000  loss: 1.2418715072373232
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6446500117778778, top 2 accuracy:0.8164000115394592

Epoch: [8 | 20]
batch_idx: 0  loss: 0.890501856803894
batch_idx: 250  loss: 1.2494718329137005
batch_idx: 500  loss: 1.2490256338408499
batch_idx: 750  loss: 1.2495396527411646
batch_idx: 1000  loss: 1.2546143304473294
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6459250135421752, top 2 accuracy:0.8180250124931335

Epoch: [9 | 20]
batch_idx: 0  loss: 1.2920494079589844
batch_idx: 250  loss: 1.2612159904122162
batch_idx: 500  loss: 1.2804800997795671
batch_idx: 750  loss: 1.2824388739909183
batch_idx: 1000  loss: 1.2955785975955165
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6460250105857849, top 2 accuracy:0.8216750118732452

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3691593408584595
batch_idx: 250  loss: 1.2774568452175548
batch_idx: 500  loss: 1.2980921480929453
batch_idx: 750  loss: 1.300284055936457
batch_idx: 1000  loss: 1.3119116414802523
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.64872501039505, top 2 accuracy:0.8232250111103058

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2744214534759521
batch_idx: 250  loss: 1.320753403991129
batch_idx: 500  loss: 1.3210415168051894
batch_idx: 750  loss: 1.3218896456574303
batch_idx: 1000  loss: 1.324431112137275
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6447250125408173, top 2 accuracy:0.8212500109672546

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1775535345077515
batch_idx: 250  loss: 1.3155691197263417
batch_idx: 500  loss: 1.326850910981496
batch_idx: 750  loss: 1.3397623606030138
batch_idx: 1000  loss: 1.3362572978432186
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.638775011062622, top 2 accuracy:0.8171750113964081

Epoch: [13 | 20]
batch_idx: 0  loss: 1.311661958694458
batch_idx: 250  loss: 1.3513904826061145
batch_idx: 500  loss: 1.351479450623955
batch_idx: 750  loss: 1.360603067232866
batch_idx: 1000  loss: 1.3567611296622517
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6471250109672546, top 2 accuracy:0.8213500123023987

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0562244653701782
batch_idx: 250  loss: 1.3781813862592882
batch_idx: 500  loss: 1.3835385269715645
batch_idx: 750  loss: 1.3894658808127054
batch_idx: 1000  loss: 1.392812786486964
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6556000113487244, top 2 accuracy:0.8256500127315521

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3857336044311523
batch_idx: 250  loss: 1.4188869448648158
batch_idx: 500  loss: 1.418236152882401
batch_idx: 750  loss: 1.4142994014045671
batch_idx: 1000  loss: 1.4130918405021722
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6475250122547149, top 2 accuracy:0.8238500113487244

Epoch: [16 | 20]
batch_idx: 0  loss: 1.311924934387207
batch_idx: 250  loss: 1.3933619895686407
batch_idx: 500  loss: 1.4147393075092747
batch_idx: 750  loss: 1.4279574611962254
batch_idx: 1000  loss: 1.4277216037050984
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6377250118255615, top 2 accuracy:0.8151000113487243

Epoch: [17 | 20]
batch_idx: 0  loss: 1.159358263015747
batch_idx: 250  loss: 1.4187803994301582
batch_idx: 500  loss: 1.4314967116765809
batch_idx: 750  loss: 1.4341564323433922
batch_idx: 1000  loss: 1.4406280957471829
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6493250124454498, top 2 accuracy:0.8188000106811524

Epoch: [18 | 20]
batch_idx: 0  loss: 1.7796087265014648
batch_idx: 250  loss: 1.475421719028006
batch_idx: 500  loss: 1.4483254140263728
batch_idx: 750  loss: 1.4532605414697628
batch_idx: 1000  loss: 1.451203942203674
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6060250110626221, top 2 accuracy:0.8019750127792359

Epoch: [19 | 20]
batch_idx: 0  loss: 1.1284013986587524
batch_idx: 250  loss: 1.4767404554000152
batch_idx: 500  loss: 1.4740406890805258
batch_idx: 750  loss: 1.4855248244472359
batch_idx: 1000  loss: 1.4882101616301475
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6380000131130219, top 2 accuracy:0.817900012254715

Epoch: [20 | 20]
batch_idx: 0  loss: 1.8988559246063232
batch_idx: 250  loss: 1.4859554295699053
batch_idx: 500  loss: 1.4846339729604159
batch_idx: 750  loss: 1.494496349516925
batch_idx: 1000  loss: 1.492350749076365
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6541000108718872, top 2 accuracy:0.8139750123023987
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6556000113487244
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.880875|0.6556000113487244
================= iter seed  1  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.68 train_acc:0.75 test_acc:0.80 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.41 train_acc:0.86 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.34 train_acc:0.89 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.30 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.27 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.26 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.24 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.20 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.18 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.88325|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.418669906822411
batch_idx: 500  loss: 1.3028633760398274
batch_idx: 750  loss: 1.2500090298657724
batch_idx: 1000  loss: 1.215858607567823
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7537000117301941, top 2 accuracy:0.8762000133991241

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9220782518386841
batch_idx: 250  loss: 1.1238867792112839
batch_idx: 500  loss: 1.1357718645290515
batch_idx: 750  loss: 1.12937588695518
batch_idx: 1000  loss: 1.1344988681494999
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7022750127315521, top 2 accuracy:0.8402250125408173

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4700766801834106
batch_idx: 250  loss: 1.1247597363302173
batch_idx: 500  loss: 1.1417685539908766
batch_idx: 750  loss: 1.1497707545852458
batch_idx: 1000  loss: 1.1550798788428687
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.676725013256073, top 2 accuracy:0.8254750101566315

Epoch: [4 | 20]
batch_idx: 0  loss: 1.3940541744232178
batch_idx: 250  loss: 1.1709601454590766
batch_idx: 500  loss: 1.171214001268101
batch_idx: 750  loss: 1.1683488696607394
batch_idx: 1000  loss: 1.1649349545851684
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6693000133037568, top 2 accuracy:0.8224250109195709

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1537532806396484
batch_idx: 250  loss: 1.1897507217995502
batch_idx: 500  loss: 1.1922505412850843
batch_idx: 750  loss: 1.1821426205206196
batch_idx: 1000  loss: 1.1870357070725186
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6646500115394592, top 2 accuracy:0.8216500120162964

Epoch: [6 | 20]
batch_idx: 0  loss: 1.3259162902832031
batch_idx: 250  loss: 1.193491036164173
batch_idx: 500  loss: 1.2017428040789646
batch_idx: 750  loss: 1.1991966284926487
batch_idx: 1000  loss: 1.211538993560087
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6680000128746033, top 2 accuracy:0.8244250121116639

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9410033822059631
batch_idx: 250  loss: 1.2508023974436835
batch_idx: 500  loss: 1.2400546633835994
batch_idx: 750  loss: 1.237544924639336
batch_idx: 1000  loss: 1.2392943058960355
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6629500117301941, top 2 accuracy:0.8222500121593476

Epoch: [8 | 20]
batch_idx: 0  loss: 0.880474328994751
batch_idx: 250  loss: 1.243023425008989
batch_idx: 500  loss: 1.245256877924647
batch_idx: 750  loss: 1.2467155074616454
batch_idx: 1000  loss: 1.2526756129182948
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6638750147819519, top 2 accuracy:0.8246500124931335

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3050957918167114
batch_idx: 250  loss: 1.2586689323234255
batch_idx: 500  loss: 1.2785703094477099
batch_idx: 750  loss: 1.280542816996511
batch_idx: 1000  loss: 1.2935992892319783
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6633500113487244, top 2 accuracy:0.8244250135421753

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3514225482940674
batch_idx: 250  loss: 1.2840183901294047
batch_idx: 500  loss: 1.2994419984174876
batch_idx: 750  loss: 1.3000269056698819
batch_idx: 1000  loss: 1.3120162599145795
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6604000113010406, top 2 accuracy:0.8202500112056732

Epoch: [11 | 20]
batch_idx: 0  loss: 1.254563570022583
batch_idx: 250  loss: 1.3242265273163922
batch_idx: 500  loss: 1.3241956236830168
batch_idx: 750  loss: 1.3238296852967026
batch_idx: 1000  loss: 1.3244370376816192
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6574250135421753, top 2 accuracy:0.816925011396408

Epoch: [12 | 20]
batch_idx: 0  loss: 1.2225613594055176
batch_idx: 250  loss: 1.3116583748348947
batch_idx: 500  loss: 1.3253873003917067
batch_idx: 750  loss: 1.3413590981073364
batch_idx: 1000  loss: 1.3392799572822767
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6554250111579895, top 2 accuracy:0.8162500121593476

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2964882850646973
batch_idx: 250  loss: 1.351051244997258
batch_idx: 500  loss: 1.3523511830509374
batch_idx: 750  loss: 1.3613044232241076
batch_idx: 1000  loss: 1.3575858204081035
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6628250114917755, top 2 accuracy:0.8180000114440918

Epoch: [14 | 20]
batch_idx: 0  loss: 1.030652642250061
batch_idx: 250  loss: 1.3795243022172743
batch_idx: 500  loss: 1.3828424789498677
batch_idx: 750  loss: 1.390679388302574
batch_idx: 1000  loss: 1.3935155150608514
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6670250113010406, top 2 accuracy:0.816750011920929

Epoch: [15 | 20]
batch_idx: 0  loss: 1.345282793045044
batch_idx: 250  loss: 1.422673077670493
batch_idx: 500  loss: 1.41930027560374
batch_idx: 750  loss: 1.4141170969346917
batch_idx: 1000  loss: 1.4148466280236032
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6577750129699707, top 2 accuracy:0.8121500120162964

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3105214834213257
batch_idx: 250  loss: 1.4009268256363316
batch_idx: 500  loss: 1.4199000073677044
batch_idx: 750  loss: 1.4336391352731666
batch_idx: 1000  loss: 1.4352813944125329
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6447250134944916, top 2 accuracy:0.7999750123023986

Epoch: [17 | 20]
batch_idx: 0  loss: 1.140853762626648
batch_idx: 250  loss: 1.4337349193842877
batch_idx: 500  loss: 1.444282812745187
batch_idx: 750  loss: 1.445385225406665
batch_idx: 1000  loss: 1.4515129399185362
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6490250117778779, top 2 accuracy:0.7988750123977661

Epoch: [18 | 20]
batch_idx: 0  loss: 1.7862942218780518
batch_idx: 250  loss: 1.4906366614355382
batch_idx: 500  loss: 1.4586426640431467
batch_idx: 750  loss: 1.461999289687483
batch_idx: 1000  loss: 1.4606321161023725
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6348000123500824, top 2 accuracy:0.7879750099182129

Epoch: [19 | 20]
batch_idx: 0  loss: 1.0890514850616455
batch_idx: 250  loss: 1.4926527382269952
batch_idx: 500  loss: 1.4898403252711137
batch_idx: 750  loss: 1.4977105587895339
batch_idx: 1000  loss: 1.5001194211669242
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6424250125885009, top 2 accuracy:0.7921750133037567

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9103302955627441
batch_idx: 250  loss: 1.5019953529482237
batch_idx: 500  loss: 1.4990943268155368
batch_idx: 750  loss: 1.508229122255761
batch_idx: 1000  loss: 1.506613850165099
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6480250113010406, top 2 accuracy:0.7868500096797943
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6670250113010406
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.88325|0.6670250113010406
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.50 train_acc:0.83 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.31 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.27 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.24 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.21 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.11 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.11 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.11 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.11 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.11 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.11 train_acc:0.96 test_acc:0.88 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4277068455192736
batch_idx: 500  loss: 1.3133632124801191
batch_idx: 750  loss: 1.2602063909434713
batch_idx: 1000  loss: 1.2255196630145415
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7496500127315521, top 2 accuracy:0.8727250127792359

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9101008772850037
batch_idx: 250  loss: 1.1332972265768126
batch_idx: 500  loss: 1.1480255013352565
batch_idx: 750  loss: 1.1401130810832012
batch_idx: 1000  loss: 1.1464782030628131
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6990250134468079, top 2 accuracy:0.8389000134468079

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4857680797576904
batch_idx: 250  loss: 1.135787411600304
batch_idx: 500  loss: 1.1550213934131786
batch_idx: 750  loss: 1.1636396969424212
batch_idx: 1000  loss: 1.1694738656615677
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.670725013256073, top 2 accuracy:0.8228000104427338

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4379624128341675
batch_idx: 250  loss: 1.1858720025121692
batch_idx: 500  loss: 1.1870059689931702
batch_idx: 750  loss: 1.1846728462244107
batch_idx: 1000  loss: 1.18163641925437
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6644750103950501, top 2 accuracy:0.8207500112056733

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1776690483093262
batch_idx: 250  loss: 1.2096759962163783
batch_idx: 500  loss: 1.2110384209304335
batch_idx: 750  loss: 1.201728882752815
batch_idx: 1000  loss: 1.2067508380443523
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6614750127792358, top 2 accuracy:0.8194000134468079

Epoch: [6 | 20]
batch_idx: 0  loss: 1.3373769521713257
batch_idx: 250  loss: 1.2101504666248073
batch_idx: 500  loss: 1.2188852756407462
batch_idx: 750  loss: 1.2170800630433436
batch_idx: 1000  loss: 1.2296898423339993
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6653750104904175, top 2 accuracy:0.8215000104904174

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9513950347900391
batch_idx: 250  loss: 1.2723571800275904
batch_idx: 500  loss: 1.258996314028994
batch_idx: 750  loss: 1.2567219275499417
batch_idx: 1000  loss: 1.258702646381558
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6592750110626221, top 2 accuracy:0.819275011062622

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9083244204521179
batch_idx: 250  loss: 1.2630621590940296
batch_idx: 500  loss: 1.2646851721657901
batch_idx: 750  loss: 1.2655994914008177
batch_idx: 1000  loss: 1.271288233347975
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6629250128269195, top 2 accuracy:0.8233500137329102

Epoch: [9 | 20]
batch_idx: 0  loss: 1.325402021408081
batch_idx: 250  loss: 1.2758237139598743
batch_idx: 500  loss: 1.2964658544204262
batch_idx: 750  loss: 1.2992193902852118
batch_idx: 1000  loss: 1.3122542397663617
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6623250110149383, top 2 accuracy:0.8248750095367432

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3595755100250244
batch_idx: 250  loss: 1.3014814179917777
batch_idx: 500  loss: 1.3170561005244035
batch_idx: 750  loss: 1.3178186897523483
batch_idx: 1000  loss: 1.3299894261950502
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6615000112056733, top 2 accuracy:0.8231000113487243

Epoch: [11 | 20]
batch_idx: 0  loss: 1.298693299293518
batch_idx: 250  loss: 1.3434944136722669
batch_idx: 500  loss: 1.3426268877880425
batch_idx: 750  loss: 1.3432423493023435
batch_idx: 1000  loss: 1.3445771623152893
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6607000093460083, top 2 accuracy:0.8200500111579895

Epoch: [12 | 20]
batch_idx: 0  loss: 1.2187308073043823
batch_idx: 250  loss: 1.3302956449018943
batch_idx: 500  loss: 1.346783239733089
batch_idx: 750  loss: 1.3619224448797866
batch_idx: 1000  loss: 1.3586879957217377
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6601750118732452, top 2 accuracy:0.8176750111579895

Epoch: [13 | 20]
batch_idx: 0  loss: 1.3256793022155762
batch_idx: 250  loss: 1.3669767066101959
batch_idx: 500  loss: 1.3678132186285807
batch_idx: 750  loss: 1.378117652317512
batch_idx: 1000  loss: 1.3742881269215015
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6636000113487244, top 2 accuracy:0.8194500122070313

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0629818439483643
batch_idx: 250  loss: 1.396340457926873
batch_idx: 500  loss: 1.3994693669595977
batch_idx: 750  loss: 1.408341801090403
batch_idx: 1000  loss: 1.4109508130259025
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.670250013589859, top 2 accuracy:0.8235500109195709

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3647823333740234
batch_idx: 250  loss: 1.4402519146095967
batch_idx: 500  loss: 1.4375934134069621
batch_idx: 750  loss: 1.432019019710567
batch_idx: 1000  loss: 1.4323186504002958
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6647750108242035, top 2 accuracy:0.8200000116825104

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3391185998916626
batch_idx: 250  loss: 1.415041857758842
batch_idx: 500  loss: 1.4354905354919616
batch_idx: 750  loss: 1.4492857668106705
batch_idx: 1000  loss: 1.4513550414540135
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6496000144481658, top 2 accuracy:0.8059750134944916

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1665685176849365
batch_idx: 250  loss: 1.444651289660904
batch_idx: 500  loss: 1.458078885430164
batch_idx: 750  loss: 1.4595992335808792
batch_idx: 1000  loss: 1.4654896155285377
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6601500127315522, top 2 accuracy:0.8081500120162964

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8311485052108765
batch_idx: 250  loss: 1.5012417271709593
batch_idx: 500  loss: 1.473222409424029
batch_idx: 750  loss: 1.4776083509486808
batch_idx: 1000  loss: 1.4758804299580022
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6382500114440918, top 2 accuracy:0.7945500123500824

Epoch: [19 | 20]
batch_idx: 0  loss: 1.1079169511795044
batch_idx: 250  loss: 1.5059478193853164
batch_idx: 500  loss: 1.5031885291780962
batch_idx: 750  loss: 1.5127841867367722
batch_idx: 1000  loss: 1.5152056235760545
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6474750123023987, top 2 accuracy:0.7989500124454498

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9360156059265137
batch_idx: 250  loss: 1.5162122664656283
batch_idx: 500  loss: 1.5130391584819203
batch_idx: 750  loss: 1.5210750826880803
batch_idx: 1000  loss: 1.5193273960211027
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6523750116825103, top 2 accuracy:0.7940000121593476
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.670250013589859
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.882325|0.670250013589859
================= iter seed  2  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.64 train_acc:0.75 test_acc:0.81 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.39 train_acc:0.87 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.33 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.29 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.27 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.25 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.24 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.88475|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4254711968720624
batch_idx: 500  loss: 1.3065082821549412
batch_idx: 750  loss: 1.2504564684841974
batch_idx: 1000  loss: 1.2150673147398061
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7342750108242035, top 2 accuracy:0.8745250127315521

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9338678121566772
batch_idx: 250  loss: 1.123304186356276
batch_idx: 500  loss: 1.1349847604783527
batch_idx: 750  loss: 1.1281716699864142
batch_idx: 1000  loss: 1.1343838190451598
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7010000095367431, top 2 accuracy:0.8406250097751617

Epoch: [3 | 20]
batch_idx: 0  loss: 1.485950231552124
batch_idx: 250  loss: 1.1251210365651712
batch_idx: 500  loss: 1.1422159855730796
batch_idx: 750  loss: 1.1504167849361548
batch_idx: 1000  loss: 1.154959018071429
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6781000123023987, top 2 accuracy:0.825275011062622

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4093438386917114
batch_idx: 250  loss: 1.169335887333547
batch_idx: 500  loss: 1.1712940668470362
batch_idx: 750  loss: 1.1697176642212352
batch_idx: 1000  loss: 1.1668305557233076
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6739000115394592, top 2 accuracy:0.823175012588501

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1585373878479004
batch_idx: 250  loss: 1.190465207035477
batch_idx: 500  loss: 1.1945433740695697
batch_idx: 750  loss: 1.1832058871598115
batch_idx: 1000  loss: 1.1896169925697695
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6721500101089477, top 2 accuracy:0.8216500113010407

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2520135641098022
batch_idx: 250  loss: 1.195085410472888
batch_idx: 500  loss: 1.2034057891159726
batch_idx: 750  loss: 1.200845712515317
batch_idx: 1000  loss: 1.2133949343055581
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6781000123023987, top 2 accuracy:0.8262000117301941

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9551070332527161
batch_idx: 250  loss: 1.2516120815315004
batch_idx: 500  loss: 1.2414239952058503
batch_idx: 750  loss: 1.238435555957238
batch_idx: 1000  loss: 1.2411888844955463
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6752750117778779, top 2 accuracy:0.8254000134468079

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9019047021865845
batch_idx: 250  loss: 1.2484935719182269
batch_idx: 500  loss: 1.2484319626429434
batch_idx: 750  loss: 1.248840223892754
batch_idx: 1000  loss: 1.2543053490618548
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6803750131130218, top 2 accuracy:0.8287250123023987

Epoch: [9 | 20]
batch_idx: 0  loss: 1.2988280057907104
batch_idx: 250  loss: 1.2618688002868372
batch_idx: 500  loss: 1.2807049558778698
batch_idx: 750  loss: 1.2832766248229466
batch_idx: 1000  loss: 1.2958658277369535
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6807000119686126, top 2 accuracy:0.8318750114440918

Epoch: [10 | 20]
batch_idx: 0  loss: 1.316545844078064
batch_idx: 250  loss: 1.2835064587229197
batch_idx: 500  loss: 1.2991509370066143
batch_idx: 750  loss: 1.3003031273926111
batch_idx: 1000  loss: 1.3125600080472974
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6780500113964081, top 2 accuracy:0.8296500124931335

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2992000579833984
batch_idx: 250  loss: 1.326283301382262
batch_idx: 500  loss: 1.3251229748961648
batch_idx: 750  loss: 1.325709401704708
batch_idx: 1000  loss: 1.326352786403685
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6796500120162964, top 2 accuracy:0.8284000120162964

Epoch: [12 | 20]
batch_idx: 0  loss: 1.214813232421875
batch_idx: 250  loss: 1.3122111787288102
batch_idx: 500  loss: 1.3271164101751987
batch_idx: 750  loss: 1.340645017079561
batch_idx: 1000  loss: 1.3387532424145994
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6769750115871429, top 2 accuracy:0.8253750109672546

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2487233877182007
batch_idx: 250  loss: 1.3517750978280327
batch_idx: 500  loss: 1.3530219307071285
batch_idx: 750  loss: 1.3621696420588856
batch_idx: 1000  loss: 1.3576268476133528
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6813750107288361, top 2 accuracy:0.82822501039505

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0746880769729614
batch_idx: 250  loss: 1.3841613984070067
batch_idx: 500  loss: 1.3874074714986142
batch_idx: 750  loss: 1.3947914588825698
batch_idx: 1000  loss: 1.3977849755329066
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6909000120162964, top 2 accuracy:0.830900012254715

Epoch: [15 | 20]
batch_idx: 0  loss: 1.409794569015503
batch_idx: 250  loss: 1.423675499963078
batch_idx: 500  loss: 1.4226934481085391
batch_idx: 750  loss: 1.4184871899948404
batch_idx: 1000  loss: 1.4192090771925716
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6871000127792358, top 2 accuracy:0.828450012922287

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3431293964385986
batch_idx: 250  loss: 1.4007368655386714
batch_idx: 500  loss: 1.4217012579266155
batch_idx: 750  loss: 1.432697629566964
batch_idx: 1000  loss: 1.4350498048023295
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6747750127315522, top 2 accuracy:0.816700010061264

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1569602489471436
batch_idx: 250  loss: 1.4341782610063902
batch_idx: 500  loss: 1.4462326404770787
batch_idx: 750  loss: 1.4467392933375027
batch_idx: 1000  loss: 1.453743233610266
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6774000115394593, top 2 accuracy:0.8161750133037567

Epoch: [18 | 20]
batch_idx: 0  loss: 1.753609538078308
batch_idx: 250  loss: 1.4867957250491992
batch_idx: 500  loss: 1.4582433884984187
batch_idx: 750  loss: 1.4623657791712235
batch_idx: 1000  loss: 1.4613722260482014
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6648250138759613, top 2 accuracy:0.8048250124454498

Epoch: [19 | 20]
batch_idx: 0  loss: 1.0937029123306274
batch_idx: 250  loss: 1.4926545881504854
batch_idx: 500  loss: 1.4903437711025158
batch_idx: 750  loss: 1.4989833123446652
batch_idx: 1000  loss: 1.5012860334100433
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6704750125408172, top 2 accuracy:0.8082250134944916

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9554502964019775
batch_idx: 250  loss: 1.5052833159132868
batch_idx: 500  loss: 1.5040921109715146
batch_idx: 750  loss: 1.5177273815651406
batch_idx: 1000  loss: 1.535803327688013
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6249750099182129, top 2 accuracy:0.7768000123500824
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6909000120162964
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.88475|0.6909000120162964
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.47 train_acc:0.84 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.30 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.26 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.24 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.11 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.11 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.10 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.10 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.10 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.10 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.10 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.10 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.10 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.10 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.10 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.10 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.09 train_acc:0.98 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.428864411500376
batch_idx: 500  loss: 1.3105774014190053
batch_idx: 750  loss: 1.2549998063937091
batch_idx: 1000  loss: 1.2196758312586777
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7205500123500824, top 2 accuracy:0.8684750118255615

Epoch: [2 | 20]
batch_idx: 0  loss: 0.8973026275634766
batch_idx: 250  loss: 1.1335584213320515
batch_idx: 500  loss: 1.1448558133564877
batch_idx: 750  loss: 1.1378819023820552
batch_idx: 1000  loss: 1.1452421576689227
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6907250108718872, top 2 accuracy:0.8347500116825104

Epoch: [3 | 20]
batch_idx: 0  loss: 1.5426485538482666
batch_idx: 250  loss: 1.1359601548631544
batch_idx: 500  loss: 1.1548598780871577
batch_idx: 750  loss: 1.1631457559474927
batch_idx: 1000  loss: 1.168389208114947
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6680250127315521, top 2 accuracy:0.8176250107288361

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4338157176971436
batch_idx: 250  loss: 1.1846830028987272
batch_idx: 500  loss: 1.1857716413110828
batch_idx: 750  loss: 1.1842171854323185
batch_idx: 1000  loss: 1.1809735184850785
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6626500124931336, top 2 accuracy:0.8158500108718872

Epoch: [5 | 20]
batch_idx: 0  loss: 1.2156846523284912
batch_idx: 250  loss: 1.2067761217467545
batch_idx: 500  loss: 1.2095357038472827
batch_idx: 750  loss: 1.1983360554259657
batch_idx: 1000  loss: 1.204531685303385
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6583750112056732, top 2 accuracy:0.8130250115394593

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2899694442749023
batch_idx: 250  loss: 1.2070925870646734
batch_idx: 500  loss: 1.2174761632793067
batch_idx: 750  loss: 1.2146036564637652
batch_idx: 1000  loss: 1.2278674265589957
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6630250105857849, top 2 accuracy:0.8170500137805938

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9722843170166016
batch_idx: 250  loss: 1.2678331569761085
batch_idx: 500  loss: 1.2568233125327686
batch_idx: 750  loss: 1.2538401915267023
batch_idx: 1000  loss: 1.256763624164243
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6633750109672546, top 2 accuracy:0.8184500122070313

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9182520508766174
batch_idx: 250  loss: 1.2654108992631181
batch_idx: 500  loss: 1.2647383162184385
batch_idx: 750  loss: 1.2649527416069373
batch_idx: 1000  loss: 1.2705220429899213
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6687500104904175, top 2 accuracy:0.8217750134468078

Epoch: [9 | 20]
batch_idx: 0  loss: 1.2921884059906006
batch_idx: 250  loss: 1.275152445976609
batch_idx: 500  loss: 1.294906143984346
batch_idx: 750  loss: 1.298410991045184
batch_idx: 1000  loss: 1.3118168253677722
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6711750109195709, top 2 accuracy:0.8248250119686127

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3535584211349487
batch_idx: 250  loss: 1.2988157942298866
batch_idx: 500  loss: 1.3153929295912503
batch_idx: 750  loss: 1.3159761989351408
batch_idx: 1000  loss: 1.3280417479503268
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.669500013589859, top 2 accuracy:0.821450011730194

Epoch: [11 | 20]
batch_idx: 0  loss: 1.3151028156280518
batch_idx: 250  loss: 1.3410192499668685
batch_idx: 500  loss: 1.3402744089112137
batch_idx: 750  loss: 1.3411699590561172
batch_idx: 1000  loss: 1.3424907822054797
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6710750126838684, top 2 accuracy:0.8213250126838684

Epoch: [12 | 20]
batch_idx: 0  loss: 1.2170392274856567
batch_idx: 250  loss: 1.3273476731796143
batch_idx: 500  loss: 1.342125287894427
batch_idx: 750  loss: 1.355826678429594
batch_idx: 1000  loss: 1.3539007893576027
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6688750116825104, top 2 accuracy:0.8199750111103058

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2558724880218506
batch_idx: 250  loss: 1.365908285676959
batch_idx: 500  loss: 1.3671818837594758
batch_idx: 750  loss: 1.37607306130964
batch_idx: 1000  loss: 1.3717957425374574
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6722500114440918, top 2 accuracy:0.8220750131607055

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0952742099761963
batch_idx: 250  loss: 1.3978692139654356
batch_idx: 500  loss: 1.401447389399606
batch_idx: 750  loss: 1.4099938784462775
batch_idx: 1000  loss: 1.4132412630148208
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6837000110149384, top 2 accuracy:0.8268000128269195

Epoch: [15 | 20]
batch_idx: 0  loss: 1.4259175062179565
batch_idx: 250  loss: 1.4349263514168502
batch_idx: 500  loss: 1.4372410844577748
batch_idx: 750  loss: 1.4336672685068932
batch_idx: 1000  loss: 1.4335228588443976
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6804500122070313, top 2 accuracy:0.8248750109672547

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3472989797592163
batch_idx: 250  loss: 1.4157776152196864
batch_idx: 500  loss: 1.435909733342592
batch_idx: 750  loss: 1.4480498309716574
batch_idx: 1000  loss: 1.450352723130022
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6657000102996826, top 2 accuracy:0.8122250134944916

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1652026176452637
batch_idx: 250  loss: 1.446150063615535
batch_idx: 500  loss: 1.4571191484563089
batch_idx: 750  loss: 1.4584288487033428
batch_idx: 1000  loss: 1.4646402092549367
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6684750130176544, top 2 accuracy:0.8140500133037567

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8185607194900513
batch_idx: 250  loss: 1.5002720378730179
batch_idx: 500  loss: 1.4716419343743028
batch_idx: 750  loss: 1.4756375255579641
batch_idx: 1000  loss: 1.4746118525441843
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6529750123023986, top 2 accuracy:0.7972000126838684

Epoch: [19 | 20]
batch_idx: 0  loss: 1.098812460899353
batch_idx: 250  loss: 1.5078549946055693
batch_idx: 500  loss: 1.5054719531745242
batch_idx: 750  loss: 1.512462915956752
batch_idx: 1000  loss: 1.5146739231035733
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6574500107765198, top 2 accuracy:0.8024500138759613

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9390637874603271
batch_idx: 250  loss: 1.5182957292929742
batch_idx: 500  loss: 1.5182074483311727
batch_idx: 750  loss: 1.529147166023234
batch_idx: 1000  loss: 1.5373102754306869
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.623650013923645, top 2 accuracy:0.7733250110149383
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6837000110149384
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.88185|0.6837000110149384
================= iter seed  3  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.63 train_acc:0.76 test_acc:0.80 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.39 train_acc:0.87 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.32 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.28 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.26 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.25 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.23 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.20 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.20 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.883175|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4277671514899248
batch_idx: 500  loss: 1.310550960532406
batch_idx: 750  loss: 1.2557407825979037
batch_idx: 1000  loss: 1.2208844508035495
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7569000103473663, top 2 accuracy:0.8728500118255615

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9302605390548706
batch_idx: 250  loss: 1.1261910478716246
batch_idx: 500  loss: 1.1394980573102809
batch_idx: 750  loss: 1.132099470634166
batch_idx: 1000  loss: 1.1377137792757905
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7005750114917755, top 2 accuracy:0.8334000117778778

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4796497821807861
batch_idx: 250  loss: 1.1293653413677065
batch_idx: 500  loss: 1.146342997203033
batch_idx: 750  loss: 1.1536244383639893
batch_idx: 1000  loss: 1.1584539524615762
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6766750118732452, top 2 accuracy:0.8192500114440918

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4118080139160156
batch_idx: 250  loss: 1.1735559181681876
batch_idx: 500  loss: 1.1751144299286593
batch_idx: 750  loss: 1.172354411101836
batch_idx: 1000  loss: 1.1692443974911215
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6684750151634217, top 2 accuracy:0.8157750105857849

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1377809047698975
batch_idx: 250  loss: 1.19178288824418
batch_idx: 500  loss: 1.19561507437598
batch_idx: 750  loss: 1.1854016714555395
batch_idx: 1000  loss: 1.1911463113114857
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6667750115394593, top 2 accuracy:0.8152500116825103

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2736520767211914
batch_idx: 250  loss: 1.1961975156408047
batch_idx: 500  loss: 1.204237126800242
batch_idx: 750  loss: 1.202609721493886
batch_idx: 1000  loss: 1.2146091221001583
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6724250111579895, top 2 accuracy:0.8203000144958497

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9603719115257263
batch_idx: 250  loss: 1.251893076684403
batch_idx: 500  loss: 1.2429924886287496
batch_idx: 750  loss: 1.2401093243283754
batch_idx: 1000  loss: 1.241875223101328
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6697500126361847, top 2 accuracy:0.8192500116825103

Epoch: [8 | 20]
batch_idx: 0  loss: 0.8986280560493469
batch_idx: 250  loss: 1.2475601506725973
batch_idx: 500  loss: 1.2483878123798249
batch_idx: 750  loss: 1.2493048105359141
batch_idx: 1000  loss: 1.2553007525805466
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6731000127792358, top 2 accuracy:0.8223250124454499

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3098032474517822
batch_idx: 250  loss: 1.2607557502573739
batch_idx: 500  loss: 1.2808173445233129
batch_idx: 750  loss: 1.2820340820041471
batch_idx: 1000  loss: 1.294570337302578
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6710500137805939, top 2 accuracy:0.8227250123023987

Epoch: [10 | 20]
batch_idx: 0  loss: 1.2968491315841675
batch_idx: 250  loss: 1.2833960748059952
batch_idx: 500  loss: 1.2983486979391776
batch_idx: 750  loss: 1.2996353789363027
batch_idx: 1000  loss: 1.3120230330636327
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.666950011253357, top 2 accuracy:0.8192500133514404

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2816015481948853
batch_idx: 250  loss: 1.323355183218545
batch_idx: 500  loss: 1.3239819950274112
batch_idx: 750  loss: 1.3235409685181074
batch_idx: 1000  loss: 1.3249624245130598
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6631500127315522, top 2 accuracy:0.8164500126838684

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1771663427352905
batch_idx: 250  loss: 1.3120076627913264
batch_idx: 500  loss: 1.325778196873277
batch_idx: 750  loss: 1.3408501165353472
batch_idx: 1000  loss: 1.3387877650487536
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6623250126838685, top 2 accuracy:0.8160250127315521

Epoch: [13 | 20]
batch_idx: 0  loss: 1.299662709236145
batch_idx: 250  loss: 1.3519271518354006
batch_idx: 500  loss: 1.3528340466474993
batch_idx: 750  loss: 1.3612086481715593
batch_idx: 1000  loss: 1.3569676416179242
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.666425010919571, top 2 accuracy:0.8175250115394592

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0528074502944946
batch_idx: 250  loss: 1.3797217344062696
batch_idx: 500  loss: 1.3828439301851263
batch_idx: 750  loss: 1.390638587860556
batch_idx: 1000  loss: 1.3932038936704492
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6732000119686127, top 2 accuracy:0.8188250110149383

Epoch: [15 | 20]
batch_idx: 0  loss: 1.381638526916504
batch_idx: 250  loss: 1.4229293959318927
batch_idx: 500  loss: 1.420792923399516
batch_idx: 750  loss: 1.413627765037837
batch_idx: 1000  loss: 1.4144479941350583
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6665250110626221, top 2 accuracy:0.8155500128269195

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3285162448883057
batch_idx: 250  loss: 1.3972367945649855
batch_idx: 500  loss: 1.418249000535247
batch_idx: 750  loss: 1.4304056175215691
batch_idx: 1000  loss: 1.4324877518720138
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6540750122070312, top 2 accuracy:0.8040750124454499

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1197928190231323
batch_idx: 250  loss: 1.4312347202119085
batch_idx: 500  loss: 1.4420065266663948
batch_idx: 750  loss: 1.4411978434856552
batch_idx: 1000  loss: 1.447484940814134
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.660225011587143, top 2 accuracy:0.8049750123023987

Epoch: [18 | 20]
batch_idx: 0  loss: 1.775195598602295
batch_idx: 250  loss: 1.487048335787981
batch_idx: 500  loss: 1.4546360537956395
batch_idx: 750  loss: 1.4599437608307761
batch_idx: 1000  loss: 1.4571832982591166
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.634975013256073, top 2 accuracy:0.7930500118732452

Epoch: [19 | 20]
batch_idx: 0  loss: 1.0938589572906494
batch_idx: 250  loss: 1.4848508537289449
batch_idx: 500  loss: 1.4841468542386471
batch_idx: 750  loss: 1.493009223399992
batch_idx: 1000  loss: 1.4943317683836141
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6482500112056733, top 2 accuracy:0.7966000106334686

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9166213274002075
batch_idx: 250  loss: 1.4980368921222293
batch_idx: 500  loss: 1.4940938250870226
batch_idx: 750  loss: 1.504553189044179
batch_idx: 1000  loss: 1.5032085231222665
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6598750102519989, top 2 accuracy:0.7948500134944916
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6732000119686127
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.883175|0.6732000119686127
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.47 train_acc:0.83 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.30 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.26 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.23 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.21 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.15 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.15 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.15 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.10 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.09 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4262326786173167
batch_idx: 500  loss: 1.3076557598330758
batch_idx: 750  loss: 1.2518170736393057
batch_idx: 1000  loss: 1.2173056880506083
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7466250109672546, top 2 accuracy:0.8690000121593475

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9013720154762268
batch_idx: 250  loss: 1.1264076866860988
batch_idx: 500  loss: 1.1383007836113705
batch_idx: 750  loss: 1.1309925175969304
batch_idx: 1000  loss: 1.137535340607928
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.687125011920929, top 2 accuracy:0.8263000123500824

Epoch: [3 | 20]
batch_idx: 0  loss: 1.5170234441757202
batch_idx: 250  loss: 1.1271837144284635
batch_idx: 500  loss: 1.1461814325391961
batch_idx: 750  loss: 1.1543095895684483
batch_idx: 1000  loss: 1.15943410718879
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.657850011587143, top 2 accuracy:0.809375011920929

Epoch: [4 | 20]
batch_idx: 0  loss: 1.459542989730835
batch_idx: 250  loss: 1.176372126858261
batch_idx: 500  loss: 1.178221644919835
batch_idx: 750  loss: 1.1752261632360506
batch_idx: 1000  loss: 1.1720858765438722
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6498250124454499, top 2 accuracy:0.8070250115394593

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1708283424377441
batch_idx: 250  loss: 1.1947912025148426
batch_idx: 500  loss: 1.1984531464474053
batch_idx: 750  loss: 1.1886945990186348
batch_idx: 1000  loss: 1.194634451152989
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6461750128269196, top 2 accuracy:0.8045500123500824

Epoch: [6 | 20]
batch_idx: 0  loss: 1.31268310546875
batch_idx: 250  loss: 1.197293755833788
batch_idx: 500  loss: 1.2074643028409857
batch_idx: 750  loss: 1.2059907154113196
batch_idx: 1000  loss: 1.2181255447740753
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6523500094413758, top 2 accuracy:0.8093000125885009

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9506339430809021
batch_idx: 250  loss: 1.2576992904059647
batch_idx: 500  loss: 1.2461899673444423
batch_idx: 750  loss: 1.244276255162997
batch_idx: 1000  loss: 1.2458131574451352
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6495250110626221, top 2 accuracy:0.8112500121593476

Epoch: [8 | 20]
batch_idx: 0  loss: 0.91041499376297
batch_idx: 250  loss: 1.2523018948982554
batch_idx: 500  loss: 1.2518754858624612
batch_idx: 750  loss: 1.2529756146710627
batch_idx: 1000  loss: 1.2582571940443006
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6543250117301941, top 2 accuracy:0.8165250103473664

Epoch: [9 | 20]
batch_idx: 0  loss: 1.2898106575012207
batch_idx: 250  loss: 1.2631862158805653
batch_idx: 500  loss: 1.2838110213644767
batch_idx: 750  loss: 1.2868653373428962
batch_idx: 1000  loss: 1.3000385711273066
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6514750125408173, top 2 accuracy:0.8153250119686126

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3432929515838623
batch_idx: 250  loss: 1.284756940669589
batch_idx: 500  loss: 1.2992545550709895
batch_idx: 750  loss: 1.3000208340850898
batch_idx: 1000  loss: 1.3124301280028903
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6504250130653382, top 2 accuracy:0.813925012588501

Epoch: [11 | 20]
batch_idx: 0  loss: 1.243378758430481
batch_idx: 250  loss: 1.3274909147966079
batch_idx: 500  loss: 1.3276086679676122
batch_idx: 750  loss: 1.3276575895431766
batch_idx: 1000  loss: 1.328978271339648
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6503500120639801, top 2 accuracy:0.8129750113487244

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1905158758163452
batch_idx: 250  loss: 1.315857138087905
batch_idx: 500  loss: 1.3292514748360362
batch_idx: 750  loss: 1.3438286568142748
batch_idx: 1000  loss: 1.3414104054339777
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6504250137805939, top 2 accuracy:0.8128250117301941

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2950516939163208
batch_idx: 250  loss: 1.3517517801877614
batch_idx: 500  loss: 1.3541739634349585
batch_idx: 750  loss: 1.3627406872923924
batch_idx: 1000  loss: 1.3592575348604221
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6527250108718872, top 2 accuracy:0.8162750120162964

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0425618886947632
batch_idx: 250  loss: 1.3810260606683873
batch_idx: 500  loss: 1.3868511705592488
batch_idx: 750  loss: 1.3950474256844898
batch_idx: 1000  loss: 1.397774883709586
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.664650013923645, top 2 accuracy:0.8207000117301941

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3777750730514526
batch_idx: 250  loss: 1.423293276223545
batch_idx: 500  loss: 1.4244538826520363
batch_idx: 750  loss: 1.4177079117951588
batch_idx: 1000  loss: 1.4183930217886505
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6564250123500824, top 2 accuracy:0.8164750134944916

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3473564386367798
batch_idx: 250  loss: 1.396088524918488
batch_idx: 500  loss: 1.4189849757311637
batch_idx: 750  loss: 1.4303978374500488
batch_idx: 1000  loss: 1.4321479656921028
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6410000126361847, top 2 accuracy:0.8049500133991242

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1187574863433838
batch_idx: 250  loss: 1.4271838458429649
batch_idx: 500  loss: 1.4389380372977523
batch_idx: 750  loss: 1.4400024857845886
batch_idx: 1000  loss: 1.4457607460907473
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6500500123500824, top 2 accuracy:0.8076000118255615

Epoch: [18 | 20]
batch_idx: 0  loss: 1.7922415733337402
batch_idx: 250  loss: 1.4833991895229903
batch_idx: 500  loss: 1.4524654131947141
batch_idx: 750  loss: 1.4603791041473178
batch_idx: 1000  loss: 1.4585605336073488
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6023000141382218, top 2 accuracy:0.7895500118732452

Epoch: [19 | 20]
batch_idx: 0  loss: 1.0914242267608643
batch_idx: 250  loss: 1.4814850036972846
batch_idx: 500  loss: 1.4798698660099145
batch_idx: 750  loss: 1.4906321989214757
batch_idx: 1000  loss: 1.4928239832480494
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6178500101566314, top 2 accuracy:0.7959000134468078

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9584341049194336
batch_idx: 250  loss: 1.491868882565885
batch_idx: 500  loss: 1.4895115204785239
batch_idx: 750  loss: 1.5005385654920462
batch_idx: 1000  loss: 1.4993620921438113
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6376250123977661, top 2 accuracy:0.7885250120162964
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.664650013923645
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.8818|0.664650013923645
================= iter seed  4  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.65 train_acc:0.75 test_acc:0.81 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.40 train_acc:0.87 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.33 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.29 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.27 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.25 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.24 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.8835|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287869215011597
batch_idx: 250  loss: 1.4159364233145844
batch_idx: 500  loss: 1.3009643634540613
batch_idx: 750  loss: 1.2485327416750394
batch_idx: 1000  loss: 1.214754511135074
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7208500113487244, top 2 accuracy:0.8637500143051148

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9270879030227661
batch_idx: 250  loss: 1.12216457899879
batch_idx: 500  loss: 1.1350012842738078
batch_idx: 750  loss: 1.1299407187454746
batch_idx: 1000  loss: 1.1363548281046147
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6962500114440918, top 2 accuracy:0.8313750121593475

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4860389232635498
batch_idx: 250  loss: 1.1278154629213064
batch_idx: 500  loss: 1.1441418580366283
batch_idx: 750  loss: 1.1515685742113297
batch_idx: 1000  loss: 1.15693132436504
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6750500130653382, top 2 accuracy:0.8204250121116639

Epoch: [4 | 20]
batch_idx: 0  loss: 1.408259391784668
batch_idx: 250  loss: 1.1737169456595646
batch_idx: 500  loss: 1.1734066981543003
batch_idx: 750  loss: 1.1715158327517832
batch_idx: 1000  loss: 1.1684325354072613
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.667850013256073, top 2 accuracy:0.8161000108718872

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1509779691696167
batch_idx: 250  loss: 1.1946779586173393
batch_idx: 500  loss: 1.1974376258952766
batch_idx: 750  loss: 1.185764042064825
batch_idx: 1000  loss: 1.1912981340774713
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6638000111579895, top 2 accuracy:0.8134750120639801

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2979564666748047
batch_idx: 250  loss: 1.1992213626544692
batch_idx: 500  loss: 1.2061595612070397
batch_idx: 750  loss: 1.2030119569521118
batch_idx: 1000  loss: 1.2157869799592245
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6706750128269195, top 2 accuracy:0.8189000108242035

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9559500217437744
batch_idx: 250  loss: 1.2526380934283161
batch_idx: 500  loss: 1.2423334528574723
batch_idx: 750  loss: 1.240460360189776
batch_idx: 1000  loss: 1.2439060983376
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6683750102519989, top 2 accuracy:0.8204250099658966

Epoch: [8 | 20]
batch_idx: 0  loss: 0.8984780311584473
batch_idx: 250  loss: 1.2505191941708562
batch_idx: 500  loss: 1.250157380693457
batch_idx: 750  loss: 1.2515638811655536
batch_idx: 1000  loss: 1.2573481631545593
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6702750113010406, top 2 accuracy:0.8231250104904175

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3404638767242432
batch_idx: 250  loss: 1.2632105918112544
batch_idx: 500  loss: 1.2837280158601119
batch_idx: 750  loss: 1.2875815582694115
batch_idx: 1000  loss: 1.2990745422653496
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6677000102996826, top 2 accuracy:0.8221250123977661

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3249598741531372
batch_idx: 250  loss: 1.2898973142209986
batch_idx: 500  loss: 1.303652434447926
batch_idx: 750  loss: 1.3045239819054149
batch_idx: 1000  loss: 1.3163280584179937
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6635750122070313, top 2 accuracy:0.8169500110149384

Epoch: [11 | 20]
batch_idx: 0  loss: 1.301864743232727
batch_idx: 250  loss: 1.3295702711576878
batch_idx: 500  loss: 1.3281453266573484
batch_idx: 750  loss: 1.3285827712533067
batch_idx: 1000  loss: 1.3289443744590488
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6628500103950501, top 2 accuracy:0.8133000113964081

Epoch: [12 | 20]
batch_idx: 0  loss: 1.2087411880493164
batch_idx: 250  loss: 1.3151863100797838
batch_idx: 500  loss: 1.3294671635593525
batch_idx: 750  loss: 1.3436936049971446
batch_idx: 1000  loss: 1.3417339142852318
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6618000113964081, top 2 accuracy:0.8128750114440918

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2636866569519043
batch_idx: 250  loss: 1.3573166078907128
batch_idx: 500  loss: 1.358825542234728
batch_idx: 750  loss: 1.3668261135176434
batch_idx: 1000  loss: 1.3622830235920014
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6670000116825103, top 2 accuracy:0.815100013256073

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0466724634170532
batch_idx: 250  loss: 1.389560115545846
batch_idx: 500  loss: 1.3910337072905552
batch_idx: 750  loss: 1.3980351838423508
batch_idx: 1000  loss: 1.4010958428057239
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6716250112056732, top 2 accuracy:0.816000009059906

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3855817317962646
batch_idx: 250  loss: 1.4272328046622829
batch_idx: 500  loss: 1.4250448140801426
batch_idx: 750  loss: 1.4208131121980059
batch_idx: 1000  loss: 1.4220739727774367
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6666250119209289, top 2 accuracy:0.8138750119209289

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3234316110610962
batch_idx: 250  loss: 1.4058543350814068
batch_idx: 500  loss: 1.4252414622565395
batch_idx: 750  loss: 1.4381176901980244
batch_idx: 1000  loss: 1.4394618089016253
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6535500109195709, top 2 accuracy:0.7994000120162964

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1712286472320557
batch_idx: 250  loss: 1.4367232726374568
batch_idx: 500  loss: 1.4491895326016622
batch_idx: 750  loss: 1.450457895671262
batch_idx: 1000  loss: 1.4566719512969921
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6546500129699707, top 2 accuracy:0.796500010728836

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8090829849243164
batch_idx: 250  loss: 1.4926693953088055
batch_idx: 500  loss: 1.4628270142006152
batch_idx: 750  loss: 1.4672372742115405
batch_idx: 1000  loss: 1.4658538409220143
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.636750010251999, top 2 accuracy:0.7863750128746033

Epoch: [19 | 20]
batch_idx: 0  loss: 1.1265103816986084
batch_idx: 250  loss: 1.4925692640541088
batch_idx: 500  loss: 1.4910797717278463
batch_idx: 750  loss: 1.5008971796129662
batch_idx: 1000  loss: 1.503673463940811
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6477250125408173, top 2 accuracy:0.7909500131607056

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9576045274734497
batch_idx: 250  loss: 1.5085859879590748
batch_idx: 500  loss: 1.505811090912355
batch_idx: 750  loss: 1.5178326366111405
batch_idx: 1000  loss: 1.518854110837935
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6403500111103058, top 2 accuracy:0.7788500120639801
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6716250112056732
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.8835|0.6716250112056732
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.48 train_acc:0.83 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.31 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.26 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.24 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.21 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.20 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.19 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.18 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.16 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4310355936969217
batch_idx: 500  loss: 1.3225373610069877
batch_idx: 750  loss: 1.2717924889190961
batch_idx: 1000  loss: 1.2381088476354323
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.704375010728836, top 2 accuracy:0.8568500142097473

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9350897669792175
batch_idx: 250  loss: 1.1535412928826858
batch_idx: 500  loss: 1.164347371891545
batch_idx: 750  loss: 1.158138837346122
batch_idx: 1000  loss: 1.1638769539781273
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6782250113487244, top 2 accuracy:0.8206750140190124

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4772402048110962
batch_idx: 250  loss: 1.1499397465648258
batch_idx: 500  loss: 1.1684871170984692
batch_idx: 750  loss: 1.1769390522386878
batch_idx: 1000  loss: 1.1822391853641017
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6547750127315521, top 2 accuracy:0.8069750125408173

Epoch: [4 | 20]
batch_idx: 0  loss: 1.431714415550232
batch_idx: 250  loss: 1.20004736022085
batch_idx: 500  loss: 1.2000624801743733
batch_idx: 750  loss: 1.1980809960167353
batch_idx: 1000  loss: 1.1949428502267923
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6498500123023987, top 2 accuracy:0.8070750122070313

Epoch: [5 | 20]
batch_idx: 0  loss: 1.200087547302246
batch_idx: 250  loss: 1.2206204888930572
batch_idx: 500  loss: 1.222598376123909
batch_idx: 750  loss: 1.2106075792188273
batch_idx: 1000  loss: 1.2170187703812847
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.642650012254715, top 2 accuracy:0.8028750121593475

Epoch: [6 | 20]
batch_idx: 0  loss: 1.3117761611938477
batch_idx: 250  loss: 1.223645341699567
batch_idx: 500  loss: 1.2307043179180444
batch_idx: 750  loss: 1.2291649450895443
batch_idx: 1000  loss: 1.242613668306567
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6476250121593475, top 2 accuracy:0.8077750101089477

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9626457095146179
batch_idx: 250  loss: 1.276941040456769
batch_idx: 500  loss: 1.2672687947179713
batch_idx: 750  loss: 1.2659291111769482
batch_idx: 1000  loss: 1.2701427627580997
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6459750106334686, top 2 accuracy:0.8094250121116638

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9413341879844666
batch_idx: 250  loss: 1.2807128574397113
batch_idx: 500  loss: 1.2788014255547258
batch_idx: 750  loss: 1.2799667261774836
batch_idx: 1000  loss: 1.2854288314430478
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6478500128984451, top 2 accuracy:0.8111000103950501

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3882989883422852
batch_idx: 250  loss: 1.2873763239251033
batch_idx: 500  loss: 1.3093112024584075
batch_idx: 750  loss: 1.314085640857802
batch_idx: 1000  loss: 1.326862651295365
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.648800012588501, top 2 accuracy:0.8142500114440918

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3972818851470947
batch_idx: 250  loss: 1.3182274223511095
batch_idx: 500  loss: 1.3320106887741332
batch_idx: 750  loss: 1.332905045400724
batch_idx: 1000  loss: 1.3434658022924735
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6429500114917756, top 2 accuracy:0.8039000101089477

Epoch: [11 | 20]
batch_idx: 0  loss: 1.3205690383911133
batch_idx: 250  loss: 1.3573396900499948
batch_idx: 500  loss: 1.3539874339788154
batch_idx: 750  loss: 1.3551843153979946
batch_idx: 1000  loss: 1.356068761037371
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6396750137805939, top 2 accuracy:0.7995500121116638

Epoch: [12 | 20]
batch_idx: 0  loss: 1.190528392791748
batch_idx: 250  loss: 1.3436023004483342
batch_idx: 500  loss: 1.3574930664264795
batch_idx: 750  loss: 1.3717768016554308
batch_idx: 1000  loss: 1.3703066849480041
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6381000106334687, top 2 accuracy:0.7977250111103058

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2987679243087769
batch_idx: 250  loss: 1.3852550056666753
batch_idx: 500  loss: 1.3870055352291613
batch_idx: 750  loss: 1.3938541586187179
batch_idx: 1000  loss: 1.3895225203551425
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6491750106811524, top 2 accuracy:0.8054750134944916

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0890812873840332
batch_idx: 250  loss: 1.4133972001568502
batch_idx: 500  loss: 1.4166492160997892
batch_idx: 750  loss: 1.425317882000607
batch_idx: 1000  loss: 1.4275316867156151
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6628750131130219, top 2 accuracy:0.8126750116348267

Epoch: [15 | 20]
batch_idx: 0  loss: 1.4280353784561157
batch_idx: 250  loss: 1.452526141425953
batch_idx: 500  loss: 1.4501174758686024
batch_idx: 750  loss: 1.4470897185288571
batch_idx: 1000  loss: 1.4483579852805732
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6550750150680542, top 2 accuracy:0.8089250104427338

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3332113027572632
batch_idx: 250  loss: 1.4315288860013262
batch_idx: 500  loss: 1.4479377615299711
batch_idx: 750  loss: 1.463177397918549
batch_idx: 1000  loss: 1.464019781127334
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6350750114917755, top 2 accuracy:0.7902250125408172

Epoch: [17 | 20]
batch_idx: 0  loss: 1.2180434465408325
batch_idx: 250  loss: 1.4646773158272044
batch_idx: 500  loss: 1.475959812149857
batch_idx: 750  loss: 1.4754044870672993
batch_idx: 1000  loss: 1.4819553796285259
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6397250118255615, top 2 accuracy:0.789600013256073

Epoch: [18 | 20]
batch_idx: 0  loss: 1.838453769683838
batch_idx: 250  loss: 1.521961116828676
batch_idx: 500  loss: 1.4917697289533782
batch_idx: 750  loss: 1.495850352672518
batch_idx: 1000  loss: 1.4921603221386766
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6154000110626221, top 2 accuracy:0.7679000117778778

Epoch: [19 | 20]
batch_idx: 0  loss: 1.1892775297164917
batch_idx: 250  loss: 1.5184910606314532
batch_idx: 500  loss: 1.51744265790191
batch_idx: 750  loss: 1.5285769105909224
batch_idx: 1000  loss: 1.5313388406754302
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6216500124931336, top 2 accuracy:0.7739250121116639

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9735761880874634
batch_idx: 250  loss: 1.5294187741742036
batch_idx: 500  loss: 1.5282141409422223
batch_idx: 750  loss: 1.536472681800757
batch_idx: 1000  loss: 1.5339418712039343
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6296500113010407, top 2 accuracy:0.7765250113010407
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6628750131130219
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.881375|0.6628750131130219
================= iter seed  5  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.67 train_acc:0.75 test_acc:0.80 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.40 train_acc:0.87 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.33 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.29 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.27 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.25 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.24 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.884525|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4250360567164155
batch_idx: 500  loss: 1.3132920917331508
batch_idx: 750  loss: 1.2618601555644102
batch_idx: 1000  loss: 1.2312797954240546
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7311250128746033, top 2 accuracy:0.8726000118255616

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9564310908317566
batch_idx: 250  loss: 1.1479127669561837
batch_idx: 500  loss: 1.1596766215001948
batch_idx: 750  loss: 1.1534160318496445
batch_idx: 1000  loss: 1.1583116694856375
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7039250125885009, top 2 accuracy:0.8354500122070313

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4733920097351074
batch_idx: 250  loss: 1.1475415862800586
batch_idx: 500  loss: 1.163170028293722
batch_idx: 750  loss: 1.1705899118679264
batch_idx: 1000  loss: 1.1755094636743442
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6821750109195709, top 2 accuracy:0.8208750107288361

Epoch: [4 | 20]
batch_idx: 0  loss: 1.403069257736206
batch_idx: 250  loss: 1.192063569833061
batch_idx: 500  loss: 1.1938714055638564
batch_idx: 750  loss: 1.1917723396345934
batch_idx: 1000  loss: 1.1888096487750641
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6739750120639801, top 2 accuracy:0.8168750128746033

Epoch: [5 | 20]
batch_idx: 0  loss: 1.195339560508728
batch_idx: 250  loss: 1.2110281638765563
batch_idx: 500  loss: 1.2143744058681256
batch_idx: 750  loss: 1.2041369967158644
batch_idx: 1000  loss: 1.2094919885309359
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6700500123500824, top 2 accuracy:0.8142500112056732

Epoch: [6 | 20]
batch_idx: 0  loss: 1.3225516080856323
batch_idx: 250  loss: 1.2173473057951572
batch_idx: 500  loss: 1.2235355343925136
batch_idx: 750  loss: 1.2215401875078202
batch_idx: 1000  loss: 1.2331888035368235
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6742250139713287, top 2 accuracy:0.817550012588501

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9617122411727905
batch_idx: 250  loss: 1.2678589576378536
batch_idx: 500  loss: 1.2577352804715554
batch_idx: 750  loss: 1.2549939519741105
batch_idx: 1000  loss: 1.2578867522243875
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6682250127792358, top 2 accuracy:0.815550011396408

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9215733408927917
batch_idx: 250  loss: 1.2640667642053625
batch_idx: 500  loss: 1.2633497327613679
batch_idx: 750  loss: 1.264444780038861
batch_idx: 1000  loss: 1.270150017838318
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6703250117301941, top 2 accuracy:0.8190250108242035

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3534839153289795
batch_idx: 250  loss: 1.2762695192722144
batch_idx: 500  loss: 1.295851101573004
batch_idx: 750  loss: 1.2977010666404143
batch_idx: 1000  loss: 1.3096442011217722
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6631250121593475, top 2 accuracy:0.8162250123023986

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3372907638549805
batch_idx: 250  loss: 1.296144234072802
batch_idx: 500  loss: 1.3120181989726838
batch_idx: 750  loss: 1.3123938398769779
batch_idx: 1000  loss: 1.3246276567633541
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.658750011920929, top 2 accuracy:0.8118500113487244

Epoch: [11 | 20]
batch_idx: 0  loss: 1.3180211782455444
batch_idx: 250  loss: 1.3370230196011275
batch_idx: 500  loss: 1.3356559846009555
batch_idx: 750  loss: 1.334890920758057
batch_idx: 1000  loss: 1.336548349275566
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6538000094890595, top 2 accuracy:0.8080500128269196

Epoch: [12 | 20]
batch_idx: 0  loss: 1.2018992900848389
batch_idx: 250  loss: 1.3226741183732764
batch_idx: 500  loss: 1.3374212551059905
batch_idx: 750  loss: 1.3512872818177402
batch_idx: 1000  loss: 1.349124219209051
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6492500121593475, top 2 accuracy:0.8040750117301941

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2929680347442627
batch_idx: 250  loss: 1.3599767490676553
batch_idx: 500  loss: 1.3621210517210254
batch_idx: 750  loss: 1.370504255653887
batch_idx: 1000  loss: 1.3665096416307714
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.652125009059906, top 2 accuracy:0.806125010728836

Epoch: [14 | 20]
batch_idx: 0  loss: 1.059145212173462
batch_idx: 250  loss: 1.388550102426441
batch_idx: 500  loss: 1.3926461271501995
batch_idx: 750  loss: 1.4009119791195328
batch_idx: 1000  loss: 1.4038538093955373
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6626000130176544, top 2 accuracy:0.8128750114440918

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3927417993545532
batch_idx: 250  loss: 1.432050310371411
batch_idx: 500  loss: 1.4285798702133519
batch_idx: 750  loss: 1.4232104438108735
batch_idx: 1000  loss: 1.4238385371506785
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.652350013256073, top 2 accuracy:0.8069750113487244

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3228085041046143
batch_idx: 250  loss: 1.4070615649033806
batch_idx: 500  loss: 1.4247495761138211
batch_idx: 750  loss: 1.4384257879074487
batch_idx: 1000  loss: 1.4392608671713942
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6351750133037567, top 2 accuracy:0.794400013923645

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1784579753875732
batch_idx: 250  loss: 1.440078623344106
batch_idx: 500  loss: 1.448895286810265
batch_idx: 750  loss: 1.4484006422133189
batch_idx: 1000  loss: 1.454977742351663
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6429750120639801, top 2 accuracy:0.7956750116348267

Epoch: [18 | 20]
batch_idx: 0  loss: 1.825683355331421
batch_idx: 250  loss: 1.4915558046680566
batch_idx: 500  loss: 1.4612241892152995
batch_idx: 750  loss: 1.4688593431506276
batch_idx: 1000  loss: 1.4682420459323036
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5927000113725662, top 2 accuracy:0.77935001039505

Epoch: [19 | 20]
batch_idx: 0  loss: 1.0903823375701904
batch_idx: 250  loss: 1.4928070234759623
batch_idx: 500  loss: 1.4873141580791565
batch_idx: 750  loss: 1.4968457877033248
batch_idx: 1000  loss: 1.499700254454209
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5990750126838684, top 2 accuracy:0.7811750123500824

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9514820575714111
batch_idx: 250  loss: 1.4966055372750626
batch_idx: 500  loss: 1.4941344749794432
batch_idx: 750  loss: 1.5029849938219566
batch_idx: 1000  loss: 1.5007119314453472
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6289500126838684, top 2 accuracy:0.7852250120639801
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6703250117301941
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.884525|0.6703250117301941
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.49 train_acc:0.83 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.30 train_acc:0.90 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.26 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.24 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.13 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.12 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287869215011597
batch_idx: 250  loss: 1.4315895409765986
batch_idx: 500  loss: 1.3226802638558108
batch_idx: 750  loss: 1.272880314987603
batch_idx: 1000  loss: 1.2418137157258515
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6879750142097473, top 2 accuracy:0.8420250141620635

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9516113996505737
batch_idx: 250  loss: 1.1605250244110301
batch_idx: 500  loss: 1.172309633980147
batch_idx: 750  loss: 1.1673342005695162
batch_idx: 1000  loss: 1.172634989308854
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6699500119686127, top 2 accuracy:0.8048750114440918

Epoch: [3 | 20]
batch_idx: 0  loss: 1.4981526136398315
batch_idx: 250  loss: 1.1597894049221458
batch_idx: 500  loss: 1.1769468981588476
batch_idx: 750  loss: 1.1828066063028755
batch_idx: 1000  loss: 1.1880708934066775
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6434000115394592, top 2 accuracy:0.7838000128269196

Epoch: [4 | 20]
batch_idx: 0  loss: 1.4225703477859497
batch_idx: 250  loss: 1.204015319980385
batch_idx: 500  loss: 1.2077901021620494
batch_idx: 750  loss: 1.206618459475935
batch_idx: 1000  loss: 1.2030846741966927
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6366500120162963, top 2 accuracy:0.7807250123023987

Epoch: [5 | 20]
batch_idx: 0  loss: 1.2271602153778076
batch_idx: 250  loss: 1.2235125139331968
batch_idx: 500  loss: 1.2275533972269421
batch_idx: 750  loss: 1.21759284662401
batch_idx: 1000  loss: 1.223427276427563
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6337250125408173, top 2 accuracy:0.7801750121116638

Epoch: [6 | 20]
batch_idx: 0  loss: 1.36886727809906
batch_idx: 250  loss: 1.2284522566393563
batch_idx: 500  loss: 1.236175604841926
batch_idx: 750  loss: 1.2349807736149108
batch_idx: 1000  loss: 1.2476140519919487
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6392000124454499, top 2 accuracy:0.7874250123500824

Epoch: [7 | 20]
batch_idx: 0  loss: 1.0012849569320679
batch_idx: 250  loss: 1.283875463971652
batch_idx: 500  loss: 1.2717616908858267
batch_idx: 750  loss: 1.2687932236388746
batch_idx: 1000  loss: 1.271246343684463
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6338000122308731, top 2 accuracy:0.7842250134944916

Epoch: [8 | 20]
batch_idx: 0  loss: 0.9397503137588501
batch_idx: 250  loss: 1.2826716294349283
batch_idx: 500  loss: 1.2791052886553738
batch_idx: 750  loss: 1.279383760903223
batch_idx: 1000  loss: 1.2846137254763716
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6350750123262405, top 2 accuracy:0.7862500123977662

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3670586347579956
batch_idx: 250  loss: 1.2866634851608822
batch_idx: 500  loss: 1.3076265647270653
batch_idx: 750  loss: 1.3107426504050879
batch_idx: 1000  loss: 1.3228963311678303
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6304250109195709, top 2 accuracy:0.7857750124931335

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3969224691390991
batch_idx: 250  loss: 1.306981106346476
batch_idx: 500  loss: 1.3242583643686638
batch_idx: 750  loss: 1.3247527387181517
batch_idx: 1000  loss: 1.337027690971431
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6269000122547149, top 2 accuracy:0.7831250128746032

Epoch: [11 | 20]
batch_idx: 0  loss: 1.3362512588500977
batch_idx: 250  loss: 1.3468656390194673
batch_idx: 500  loss: 1.3468219275679885
batch_idx: 750  loss: 1.3473387005292334
batch_idx: 1000  loss: 1.3496241708771108
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6251000125408173, top 2 accuracy:0.780200011730194

Epoch: [12 | 20]
batch_idx: 0  loss: 1.184314489364624
batch_idx: 250  loss: 1.336539869941475
batch_idx: 500  loss: 1.3501616908887928
batch_idx: 750  loss: 1.3651234123096496
batch_idx: 1000  loss: 1.3623091898644313
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6212000122070312, top 2 accuracy:0.7750250108242035

Epoch: [13 | 20]
batch_idx: 0  loss: 1.3134491443634033
batch_idx: 250  loss: 1.3723747985162114
batch_idx: 500  loss: 1.3729031222383752
batch_idx: 750  loss: 1.3817110727479192
batch_idx: 1000  loss: 1.3779363514849554
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6206250114440918, top 2 accuracy:0.775900010585785

Epoch: [14 | 20]
batch_idx: 0  loss: 1.075043797492981
batch_idx: 250  loss: 1.3997324887064948
batch_idx: 500  loss: 1.402571789670789
batch_idx: 750  loss: 1.4105669460504724
batch_idx: 1000  loss: 1.4138080175406635
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6279750120639801, top 2 accuracy:0.7858000118732452

Epoch: [15 | 20]
batch_idx: 0  loss: 1.4081733226776123
batch_idx: 250  loss: 1.444435172316001
batch_idx: 500  loss: 1.443141758061672
batch_idx: 750  loss: 1.4371913920824295
batch_idx: 1000  loss: 1.4367062762712899
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.616200012922287, top 2 accuracy:0.7804000115394593

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3429542779922485
batch_idx: 250  loss: 1.4163073017032424
batch_idx: 500  loss: 1.4378043323802796
batch_idx: 750  loss: 1.450459120183501
batch_idx: 1000  loss: 1.4498148849025703
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5828250126838684, top 2 accuracy:0.7626000108718872

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1910297870635986
batch_idx: 250  loss: 1.4386228207374414
batch_idx: 500  loss: 1.4508730043824971
batch_idx: 750  loss: 1.4526496396983413
batch_idx: 1000  loss: 1.4598939220031228
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.5985250120162964, top 2 accuracy:0.7621500098705292

Epoch: [18 | 20]
batch_idx: 0  loss: 1.7857310771942139
batch_idx: 250  loss: 1.5016604764283368
batch_idx: 500  loss: 1.4746793829843379
batch_idx: 750  loss: 1.4950177236209847
batch_idx: 1000  loss: 1.5050767636099183
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.4890250091552734, top 2 accuracy:0.7411000108718873

Epoch: [19 | 20]
batch_idx: 0  loss: 1.146834373474121
batch_idx: 250  loss: 1.5687581344325516
batch_idx: 500  loss: 1.563664578935176
batch_idx: 750  loss: 1.5808255241677251
batch_idx: 1000  loss: 1.5875686997422775
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.3679750053882599, top 2 accuracy:0.7381500132083892

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9609031677246094
batch_idx: 250  loss: 1.5985109159791033
batch_idx: 500  loss: 1.6000483944845731
batch_idx: 750  loss: 1.6151050025316678
batch_idx: 1000  loss: 1.6164424808356708
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.31250000566244124, top 2 accuracy:0.732575012922287
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6350750123262405
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.881775|0.6350750123262405
