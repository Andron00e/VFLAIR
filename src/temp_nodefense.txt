================= iter seed  60  =================
===== No Defense ======
running on cuda3
============ apply_trainable_layer= 0 ============
======= Defense ========
Defense_Name: None
Defense_Config: None
===== Total Attack Tested: 2  ======
targeted_backdoor: [] []
untargeted_backdoor: [] []
label_inference: ['PassiveModelCompletion', 'ActiveModelCompletion'] [0, 1]
attribute_inference: [] []
feature_inference: [] []
exp_result/nuswide/Q1/0/None_None,model=MLP2.txt
=================================

No Attack==============================
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.65 train_acc:0.75 test_acc:0.80 test_auc:0.96
validate and test
Epoch 1% 	 train_loss:0.40 train_acc:0.88 test_acc:0.87 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.33 train_acc:0.89 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.29 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.27 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.26 train_acc:0.92 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.24 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.22 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.18 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.17 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.11 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|acc_loss,2|1024|0.003000|5|1|0|100|No_Attack|None|0.883725|0.0
======= Test Attack 0 :  PassiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287869215011597
batch_idx: 250  loss: 1.4198507402204747
batch_idx: 500  loss: 1.3004951907211895
batch_idx: 750  loss: 1.245069120652248
batch_idx: 1000  loss: 1.2092067614530984
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7513500106334686, top 2 accuracy:0.8737750113010406

Epoch: [2 | 20]
batch_idx: 0  loss: 0.9149290323257446
batch_idx: 250  loss: 1.1175668930400915
batch_idx: 500  loss: 1.1297053048295078
batch_idx: 750  loss: 1.1229073016677784
batch_idx: 1000  loss: 1.1281196773052216
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7028750109672547, top 2 accuracy:0.8378250102996826

Epoch: [3 | 20]
batch_idx: 0  loss: 1.471775770187378
batch_idx: 250  loss: 1.1186669827076134
batch_idx: 500  loss: 1.1355507023977132
batch_idx: 750  loss: 1.1440115119625496
batch_idx: 1000  loss: 1.1490242156072166
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6815500123500824, top 2 accuracy:0.8259250118732452

Epoch: [4 | 20]
batch_idx: 0  loss: 1.396235704421997
batch_idx: 250  loss: 1.1657419393286228
batch_idx: 500  loss: 1.1690789789579321
batch_idx: 750  loss: 1.166359381291256
batch_idx: 1000  loss: 1.163072081181569
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6774750139713287, top 2 accuracy:0.824075012922287

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1693131923675537
batch_idx: 250  loss: 1.1855875318114821
batch_idx: 500  loss: 1.190230392905894
batch_idx: 750  loss: 1.180089993797889
batch_idx: 1000  loss: 1.1857359756629307
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6756250107288361, top 2 accuracy:0.8234250113964081

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2891143560409546
batch_idx: 250  loss: 1.1908979113606089
batch_idx: 500  loss: 1.1996513927857462
batch_idx: 750  loss: 1.197714562870328
batch_idx: 1000  loss: 1.2111956027988047
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.679825011730194, top 2 accuracy:0.826950010061264

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9525107741355896
batch_idx: 250  loss: 1.249517055687351
batch_idx: 500  loss: 1.2401741682247303
batch_idx: 750  loss: 1.236938132638307
batch_idx: 1000  loss: 1.2395889142546028
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.676950011253357, top 2 accuracy:0.8235500118732453

Epoch: [8 | 20]
batch_idx: 0  loss: 0.8751471042633057
batch_idx: 250  loss: 1.2456421248484493
batch_idx: 500  loss: 1.2465933126886115
batch_idx: 750  loss: 1.2461500241757708
batch_idx: 1000  loss: 1.2522105825261567
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6802250120639801, top 2 accuracy:0.8280250134468079

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3447901010513306
batch_idx: 250  loss: 1.2595748225721533
batch_idx: 500  loss: 1.2786148983801
batch_idx: 750  loss: 1.2801566551250623
batch_idx: 1000  loss: 1.2935698064990317
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6773500127792358, top 2 accuracy:0.8258250117301941

Epoch: [10 | 20]
batch_idx: 0  loss: 1.335041880607605
batch_idx: 250  loss: 1.2794826177610692
batch_idx: 500  loss: 1.2973564429241313
batch_idx: 750  loss: 1.2990835729020906
batch_idx: 1000  loss: 1.3107629638557998
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6734250113964081, top 2 accuracy:0.8218750123977661

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2762376070022583
batch_idx: 250  loss: 1.325417188373772
batch_idx: 500  loss: 1.3250943863886204
batch_idx: 750  loss: 1.3248118604352463
batch_idx: 1000  loss: 1.3247557311488416
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6707750115394592, top 2 accuracy:0.8183750123977661

Epoch: [12 | 20]
batch_idx: 0  loss: 1.187429666519165
batch_idx: 250  loss: 1.3130357215249102
batch_idx: 500  loss: 1.3267732516905528
batch_idx: 750  loss: 1.3411759043894529
batch_idx: 1000  loss: 1.338386849973339
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6683750133514405, top 2 accuracy:0.8156500108242035

Epoch: [13 | 20]
batch_idx: 0  loss: 1.2919365167617798
batch_idx: 250  loss: 1.3528881283367382
batch_idx: 500  loss: 1.3528330465347953
batch_idx: 750  loss: 1.361223771509939
batch_idx: 1000  loss: 1.3569085265929326
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6728000106811524, top 2 accuracy:0.8170500111579895

Epoch: [14 | 20]
batch_idx: 0  loss: 1.0206325054168701
batch_idx: 250  loss: 1.376836509317965
batch_idx: 500  loss: 1.381158015564488
batch_idx: 750  loss: 1.389317667820531
batch_idx: 1000  loss: 1.3932278007268906
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6779750115871429, top 2 accuracy:0.8189000089168549

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3524909019470215
batch_idx: 250  loss: 1.42113033245023
batch_idx: 500  loss: 1.4196536779118496
batch_idx: 750  loss: 1.4147304455418102
batch_idx: 1000  loss: 1.4143881171989365
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6723750126361847, top 2 accuracy:0.8159250130653382

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3113722801208496
batch_idx: 250  loss: 1.4018398710009783
batch_idx: 500  loss: 1.4209514221791446
batch_idx: 750  loss: 1.4328037093394992
batch_idx: 1000  loss: 1.4358174825390688
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6629250118732453, top 2 accuracy:0.8066750111579895

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1392779350280762
batch_idx: 250  loss: 1.4355622596316193
batch_idx: 500  loss: 1.4463164556349675
batch_idx: 750  loss: 1.4462544807669582
batch_idx: 1000  loss: 1.4517521357383971
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6621250102519989, top 2 accuracy:0.803000011920929

Epoch: [18 | 20]
batch_idx: 0  loss: 1.7954113483428955
batch_idx: 250  loss: 1.4903818677832477
batch_idx: 500  loss: 1.4576729079753017
batch_idx: 750  loss: 1.460334966828558
batch_idx: 1000  loss: 1.4587977486677444
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6491500134468079, top 2 accuracy:0.7886250116825104

Epoch: [19 | 20]
batch_idx: 0  loss: 1.158164620399475
batch_idx: 250  loss: 1.4914809516579244
batch_idx: 500  loss: 1.4882649942828519
batch_idx: 750  loss: 1.4983267284822692
batch_idx: 1000  loss: 1.5010629691921484
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6567750113010407, top 2 accuracy:0.794300011396408

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9433770179748535
batch_idx: 250  loss: 1.5007369851838614
batch_idx: 500  loss: 1.4958742957746394
batch_idx: 750  loss: 1.5043211464808304
batch_idx: 1000  loss: 1.5026087357213322
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6637500123977661, top 2 accuracy:0.7957250113487244
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6802250120639801
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|PassiveModelCompletion|None|0.883725|0.6802250120639801
======= Test Attack 1 :  ActiveModelCompletion  =======
attack configs: {'party': [0], 'val_iteration': 1024, 'epochs': 20, 'batch_size': 16, 'lr': 0.002, 'n_labeled_per_class': 4}
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
AMC: use Malicious optimizer for party 0
Load Defense models
load_dataset_per_party  args.need_auxiliary =  0
nuswide dataset [train]: torch.Size([60000, 1000]) torch.Size([60000, 634]) torch.Size([60000, 5])
nuswide dataset [test]: torch.Size([40000, 1000]) torch.Size([40000, 634]) torch.Size([40000, 5])
current_model_type=MLP2
Load Defense models
validate and test
Epoch 0% 	 train_loss:0.49 train_acc:0.84 test_acc:0.85 test_auc:0.97
validate and test
Epoch 1% 	 train_loss:0.31 train_acc:0.91 test_acc:0.88 test_auc:0.98
validate and test
Epoch 2% 	 train_loss:0.26 train_acc:0.92 test_acc:0.88 test_auc:0.98
validate and test
Epoch 3% 	 train_loss:0.24 train_acc:0.93 test_acc:0.88 test_auc:0.98
validate and test
Epoch 4% 	 train_loss:0.23 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 5% 	 train_loss:0.22 train_acc:0.93 test_acc:0.89 test_auc:0.98
validate and test
Epoch 6% 	 train_loss:0.21 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 7% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 8% 	 train_loss:0.20 train_acc:0.94 test_acc:0.89 test_auc:0.98
validate and test
Epoch 9% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 10% 	 train_loss:0.19 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 11% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 12% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 13% 	 train_loss:0.18 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 14% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 15% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 16% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 17% 	 train_loss:0.17 train_acc:0.95 test_acc:0.89 test_auc:0.98
validate and test
Epoch 18% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 19% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 20% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 21% 	 train_loss:0.16 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 22% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 23% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 24% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 25% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 26% 	 train_loss:0.15 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 27% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 28% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 29% 	 train_loss:0.14 train_acc:0.96 test_acc:0.89 test_auc:0.98
validate and test
Epoch 30% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 31% 	 train_loss:0.14 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 32% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 33% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 34% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 35% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 36% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 37% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 38% 	 train_loss:0.13 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 39% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 40% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 41% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 42% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 43% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 44% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 45% 	 train_loss:0.12 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 46% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 47% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 48% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 49% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 50% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 51% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 52% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 53% 	 train_loss:0.11 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 54% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 55% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 56% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 57% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 58% 	 train_loss:0.10 train_acc:0.97 test_acc:0.89 test_auc:0.98
validate and test
Epoch 59% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 60% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 61% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 62% 	 train_loss:0.10 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 63% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 64% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 65% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 66% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 67% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 68% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 69% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 70% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 71% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 72% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 73% 	 train_loss:0.09 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 74% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 75% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 76% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 77% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 78% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 79% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 80% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 81% 	 train_loss:0.08 train_acc:0.97 test_acc:0.88 test_auc:0.98
validate and test
Epoch 82% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 83% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 84% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 85% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 86% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 87% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 88% 	 train_loss:0.08 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 89% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 90% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 91% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 92% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 93% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 94% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 95% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 96% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 97% 	 train_loss:0.07 train_acc:0.99 test_acc:0.88 test_auc:0.98
validate and test
Epoch 98% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
validate and test
Epoch 99% 	 train_loss:0.07 train_acc:0.98 test_acc:0.88 test_auc:0.98
all_train_data: torch.Size([60000, 1000])
all_train_label: torch.Size([60000, 5])
train_data: torch.Size([59980, 1000])
batch_size: 16
MC Attack, self.device=cuda

Epoch: [1 | 20]
batch_idx: 0  loss: 1.6287868022918701
batch_idx: 250  loss: 1.4141317243037808
batch_idx: 500  loss: 1.290635766357516
batch_idx: 750  loss: 1.2324242129358858
batch_idx: 1000  loss: 1.1962981909132613
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.751525011062622, top 2 accuracy:0.8759750103950501

Epoch: [2 | 20]
batch_idx: 0  loss: 0.8770732879638672
batch_idx: 250  loss: 1.1057638800769618
batch_idx: 500  loss: 1.1195059302701145
batch_idx: 750  loss: 1.111954267130309
batch_idx: 1000  loss: 1.117884573940271
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.7015250115394592, top 2 accuracy:0.8441500120162964

Epoch: [3 | 20]
batch_idx: 0  loss: 1.5173922777175903
batch_idx: 250  loss: 1.109166858685225
batch_idx: 500  loss: 1.1281572291820623
batch_idx: 750  loss: 1.137309589146172
batch_idx: 1000  loss: 1.1427445738507918
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6779750120639801, top 2 accuracy:0.828950012922287

Epoch: [4 | 20]
batch_idx: 0  loss: 1.3863098621368408
batch_idx: 250  loss: 1.15810914136268
batch_idx: 500  loss: 1.1634612763137149
batch_idx: 750  loss: 1.161896787500559
batch_idx: 1000  loss: 1.158455127415756
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6744500143527985, top 2 accuracy:0.8262000119686127

Epoch: [5 | 20]
batch_idx: 0  loss: 1.1894742250442505
batch_idx: 250  loss: 1.1832297110784982
batch_idx: 500  loss: 1.1873476076544376
batch_idx: 750  loss: 1.176207129754812
batch_idx: 1000  loss: 1.182230439406043
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.67272501206398, top 2 accuracy:0.8237250118255616

Epoch: [6 | 20]
batch_idx: 0  loss: 1.2893190383911133
batch_idx: 250  loss: 1.190172877713493
batch_idx: 500  loss: 1.2003971286937951
batch_idx: 750  loss: 1.1985869909170273
batch_idx: 1000  loss: 1.2123634560277667
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6781750130653381, top 2 accuracy:0.8292000107765197

Epoch: [7 | 20]
batch_idx: 0  loss: 0.9399644136428833
batch_idx: 250  loss: 1.2478228557280402
batch_idx: 500  loss: 1.2373530156494519
batch_idx: 750  loss: 1.2344068047402703
batch_idx: 1000  loss: 1.237818876037392
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6757500123977661, top 2 accuracy:0.8284500122070313

Epoch: [8 | 20]
batch_idx: 0  loss: 0.8780360221862793
batch_idx: 250  loss: 1.24828285414767
batch_idx: 500  loss: 1.2480472017798507
batch_idx: 750  loss: 1.2469025679634504
batch_idx: 1000  loss: 1.2522772925492294
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.680600013256073, top 2 accuracy:0.8321750118732453

Epoch: [9 | 20]
batch_idx: 0  loss: 1.3631360530853271
batch_idx: 250  loss: 1.2593233633685756
batch_idx: 500  loss: 1.2800581687660308
batch_idx: 750  loss: 1.2818125013406763
batch_idx: 1000  loss: 1.29530170029059
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6789500119686127, top 2 accuracy:0.8301500117778778

Epoch: [10 | 20]
batch_idx: 0  loss: 1.3724696636199951
batch_idx: 250  loss: 1.2794899190931517
batch_idx: 500  loss: 1.2970414850415226
batch_idx: 750  loss: 1.2998331298594654
batch_idx: 1000  loss: 1.3119064066023491
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.677825011730194, top 2 accuracy:0.8274500112533569

Epoch: [11 | 20]
batch_idx: 0  loss: 1.2494597434997559
batch_idx: 250  loss: 1.328878973076188
batch_idx: 500  loss: 1.3265040134223836
batch_idx: 750  loss: 1.3277183827597643
batch_idx: 1000  loss: 1.3274335508434156
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6756500108242035, top 2 accuracy:0.8245750114917755

Epoch: [12 | 20]
batch_idx: 0  loss: 1.1604299545288086
batch_idx: 250  loss: 1.3134810257412861
batch_idx: 500  loss: 1.3267536679523413
batch_idx: 750  loss: 1.3421544121452442
batch_idx: 1000  loss: 1.3392941484245628
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6711750128269196, top 2 accuracy:0.8211250119209289

Epoch: [13 | 20]
batch_idx: 0  loss: 1.281945824623108
batch_idx: 250  loss: 1.3547885797929689
batch_idx: 500  loss: 1.3540451926382724
batch_idx: 750  loss: 1.3631264877928388
batch_idx: 1000  loss: 1.3590604113504148
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6768750128746033, top 2 accuracy:0.8237750120162964

Epoch: [14 | 20]
batch_idx: 0  loss: 1.016050100326538
batch_idx: 250  loss: 1.3799025525349313
batch_idx: 500  loss: 1.384076432415553
batch_idx: 750  loss: 1.3917402502004614
batch_idx: 1000  loss: 1.395219345657399
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6826000108718872, top 2 accuracy:0.8263000128269196

Epoch: [15 | 20]
batch_idx: 0  loss: 1.3441413640975952
batch_idx: 250  loss: 1.422234636801036
batch_idx: 500  loss: 1.4208598629329383
batch_idx: 750  loss: 1.415681620237991
batch_idx: 1000  loss: 1.4159201753025237
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6743000116348267, top 2 accuracy:0.8201500110626221

Epoch: [16 | 20]
batch_idx: 0  loss: 1.3254157304763794
batch_idx: 250  loss: 1.4024233797206789
batch_idx: 500  loss: 1.4224563237202414
batch_idx: 750  loss: 1.435662542174889
batch_idx: 1000  loss: 1.4373558428102788
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6628000123500823, top 2 accuracy:0.8065000116825104

Epoch: [17 | 20]
batch_idx: 0  loss: 1.1123085021972656
batch_idx: 250  loss: 1.430683613581574
batch_idx: 500  loss: 1.445219275673801
batch_idx: 750  loss: 1.4459826103736264
batch_idx: 1000  loss: 1.4513385870967048
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6642750136852265, top 2 accuracy:0.802375011920929

Epoch: [18 | 20]
batch_idx: 0  loss: 1.8508362770080566
batch_idx: 250  loss: 1.4918364644619164
batch_idx: 500  loss: 1.4590016870597522
batch_idx: 750  loss: 1.4618737858746393
batch_idx: 1000  loss: 1.4616588226998577
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6494750111103058, top 2 accuracy:0.7900000116825103

Epoch: [19 | 20]
batch_idx: 0  loss: 1.174941897392273
batch_idx: 250  loss: 1.4964743312477875
batch_idx: 500  loss: 1.4928095976701763
batch_idx: 750  loss: 1.5023699440557694
batch_idx: 1000  loss: 1.5032556780134907
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.657900012254715, top 2 accuracy:0.7980250105857849

Epoch: [20 | 20]
batch_idx: 0  loss: 1.9349820613861084
batch_idx: 250  loss: 1.5011051500923873
batch_idx: 500  loss: 1.4976864512267105
batch_idx: 750  loss: 1.5052095454953465
batch_idx: 1000  loss: 1.5034729544394694
---MC: Label inference on test dataset:
Dataset Overall Statistics:
top 1 accuracy:0.6571500117778778, top 2 accuracy:0.793825012922287
MC, if self.args.apply_defense=False
MC Best top 1 accuracy: 0.6826000108718872
returning from PMC/AMC
K|bs|LR|num_class|Q|top_trainable|epoch|attack_name|None|main_task_acc|label_recovery_rate,2|1024|0.003000|5|1|0|100|ActiveModelCompletion|None|0.88175|0.6826000108718872
